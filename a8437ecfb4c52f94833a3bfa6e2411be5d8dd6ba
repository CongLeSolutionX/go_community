{
  "comments": [
    {
      "key": {
        "uuid": "0e6ac0ea_30c46bed",
        "filename": "src/sync/waitgroup.go",
        "patchSetId": 1
      },
      "lineNbr": 66,
      "author": {
        "id": 6365
      },
      "writtenOn": "2017-11-29T21:32:46Z",
      "side": 1,
      "message": "If this panic is recovered, it leaves the WaitGroup in an inconsistent state: statep\u003c\u003c32 has already been adjusted, and may have been observed or even modified by some other concurrent call to Add or Wait.\n\nUnlike the other panics in this function, nothing in the documentation prohibits user code from calling Add with a too-large value (or, more subtly, with a sum of values that sum to too-large).\n\nI think a proper fix for #20687 either needs to include explicit documentation about range limitations or ensure that the WaitGroup remains in a consistent state both before and after the panic.",
      "revId": "a8437ecfb4c52f94833a3bfa6e2411be5d8dd6ba",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "9dae2bbf_59d3b8d6",
        "filename": "src/sync/waitgroup.go",
        "patchSetId": 1
      },
      "lineNbr": 66,
      "author": {
        "id": 5167
      },
      "writtenOn": "2017-11-30T21:08:39Z",
      "side": 1,
      "message": "I think this is a good point, but I\u0027m not sure how to keep the WaitGroup in a consistent state without turning this into a CAS loop, which I\u0027d be worried about the performance effects of.\n\nWe could take a few more bits away from the maximum allowed delta if we\u0027re willing to assume that no more than 2^bits goroutines will simultaneously try to overflow the WaitGroup. Then we could let the Add \"overflow\" without actually wrapping and then safely undo the operation. Or, it\u0027s probably better to steal the bits from the waiter count in the low 32 bits, since that\u0027s more bounded than the count, and already involves a CAS loop that would make it easy to detect overflow in.",
      "parentUuid": "0e6ac0ea_30c46bed",
      "revId": "a8437ecfb4c52f94833a3bfa6e2411be5d8dd6ba",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "b4ba8d0e_0b2cf091",
        "filename": "src/sync/waitgroup.go",
        "patchSetId": 1
      },
      "lineNbr": 66,
      "author": {
        "id": 5167
      },
      "writtenOn": "2017-11-30T21:24:19Z",
      "side": 1,
      "message": "Russ points out that some of the panics here also leave the WaitGroup in a bad state, so it\u0027s not clear that adding one more is that big of a deal.\n\nTwo more approaches are:\n\n3. Poison the WaitGroup so all future operations (and maybe blocked waiters?), panic. I think doing that safely (in the sense of \"never allow Wait to unblock prematurely\") still requires either a CAS loop here or something like I said above about adding some spill bits, but that\u0027s fine.\n\n4. Make this a throw instead of a panic. We already do that in the rwmutex code when the user does something that breaks it beyond repair. In fact, when we did that we had a very similar discussion and decided it wasn\u0027t worth performance impact to the common case in order to enable dubious recovery of a sync misuse.\n\nAll of these are probably 1.11 material, though we could get it queued up now.",
      "parentUuid": "9dae2bbf_59d3b8d6",
      "revId": "a8437ecfb4c52f94833a3bfa6e2411be5d8dd6ba",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    }
  ]
}