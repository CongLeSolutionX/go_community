// Copyright 2009 The Go Authors.  All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

// Garbage collector: type and heap bitmaps.
//
// Stack, data, and bss bitmaps
//
// Stack frames and global variables in the data and bss sections are described
// by 1-bit bitmaps in which 0 means uninteresting and 1 means live pointer
// to be visited during GC. The bits in each byte are consumed starting with
// the low bit: 1<<0, 1<<1, and so on.
//
// Heap bitmap
//
// The allocated heap comes from a subset of the memory in the range [start, used),
// where start == mheap_.arena_start and used == mheap_.arena_used.
// The heap bitmap comprises 2 bits for each pointer-sized word in that range,
// stored in bytes indexed backward in memory from start.
// That is, the byte at address start-1 holds the 2-bit entries for the four words
// start through start+3*ptrSize, the byte at start-2 holds the entries for
// start+4*ptrSize through start+7*ptrSize, and so on.
//
// In each 2-bit entry, the lower bit holds the same information as in the 1-bit
// bitmaps: 0 means uninteresting and 1 means live pointer to be visited during GC.
// The meaning of the high bit depends on the position of the word being described
// in its allocated object. In the first word, the high bit is the GC ``marked'' bit.
// In the second word, the high bit is the GC ``checkmarked'' bit (see below).
// In the third and later words, the high bit indicates that the object is still
// being described. In these words, if a bit pair with a high bit 0 is encountered,
// the low bit can also be assumed to be 0, and the object description is over.
// This 00 is called the ``dead'' encoding: it signals that the rest of the words
// in the object are uninteresting to the garbage collector.
//
// The 2-bit entries are split when written into the byte, so that the top half
// of the byte contains 4 mark bits and the bottom half contains 4 pointer bits.
// This form allows a copy from the 1-bit to the 4-bit form to keep the
// pointer bits contiguous, instead of having to space them out.
//
// The code makes use of the fact that the zero value for a heap bitmap
// has no live pointer bit set and is (depending on position), not marked,
// not checkmarked, and is the dead encoding.
// These properties must be preserved when modifying the encoding.
//
// Checkmarks
//
// In a concurrent garbage collector, one worries about failing to mark
// a live object due to mutations without write barriers or bugs in the
// collector implementation. As a sanity check, the GC has a 'checkmark'
// mode that retraverses the object graph with the world stopped, to make
// sure that everything that should be marked is marked.
// In checkmark mode, in the heap bitmap, the high bit of the 2-bit entry
// for the second word of the object holds the checkmark bit.
// When not in checkmark mode, this bit is set to 1.
//
// The smallest possible allocation is 8 bytes. On a 32-bit machine, that
// means every allocated object has two words, so there is room for the
// checkmark bit. On a 64-bit machine, however, the 8-byte allocation is
// just one word, so the second bit pair is not available for encoding the
// checkmark. However, because non-pointer allocations are combined
// into larger 16-byte (maxTinySize) allocations, a plain 8-byte allocation
// must be a pointer, so the type bit in the first word is not actually needed.
// It is still used in general, except in checkmark the type bit is repurposed
// as the checkmark bit and then reinitialized (to 1) as the type bit when
// finished.

package base

import (
	"unsafe"
)

const (
	BitPointer = 1 << 0
	BitMarked  = 1 << 4

	HeapBitsShift   = 1                 // shift offset between successive bitPointer or bitMarked entries
	HeapBitmapScale = PtrSize * (8 / 2) // number of data bytes described by one heap bitmap byte

	// all mark/pointer bits in a byte
	BitMarkedAll  = BitMarked | BitMarked<<HeapBitsShift | BitMarked<<(2*HeapBitsShift) | BitMarked<<(3*HeapBitsShift)
	BitPointerAll = BitPointer | BitPointer<<HeapBitsShift | BitPointer<<(2*HeapBitsShift) | BitPointer<<(3*HeapBitsShift)
)

// subtractb returns the byte pointer p-n.
//go:nowritebarrier
func Subtractb(p *byte, n uintptr) *byte {
	// Note: wrote out full expression instead of calling add(p, -n)
	// to reduce the number of temporaries generated by the
	// compiler for this trivial expression during inlining.
	return (*byte)(unsafe.Pointer(uintptr(unsafe.Pointer(p)) - n))
}

// add1 returns the byte pointer p+1.
//go:nowritebarrier
func Add1(p *byte) *byte {
	// Note: wrote out full expression instead of calling addb(p, 1)
	// to reduce the number of temporaries generated by the
	// compiler for this trivial expression during inlining.
	return (*byte)(unsafe.Pointer(uintptr(unsafe.Pointer(p)) + 1))
}

// subtract1 returns the byte pointer p-1.
//go:nowritebarrier
func Subtract1(p *byte) *byte {
	// Note: wrote out full expression instead of calling subtractb(p, 1)
	// to reduce the number of temporaries generated by the
	// compiler for this trivial expression during inlining.
	return (*byte)(unsafe.Pointer(uintptr(unsafe.Pointer(p)) - 1))
}

// mHeap_MapBits is called each time arena_used is extended.
// It maps any additional bitmap memory needed for the new arena memory.
// It must be called with the expected new value of arena_used,
// *before* h.arena_used has been updated.
// Waiting to update arena_used until after the memory has been mapped
// avoids faults when other threads try access the bitmap immediately
// after observing the change to arena_used.
//
//go:nowritebarrier
func mHeap_MapBits(h *Mheap, arena_used uintptr) {
	// Caller has added extra mappings to the arena.
	// Add extra mappings of bitmap words as needed.
	// We allocate extra bitmap pieces in chunks of bitmapChunk.
	const bitmapChunk = 8192

	n := (arena_used - Mheap_.Arena_start) / HeapBitmapScale
	n = Round(n, bitmapChunk)
	n = Round(n, PhysPageSize)
	if h.bitmap_mapped >= n {
		return
	}

	sysMap(unsafe.Pointer(h.Arena_start-n), n-h.bitmap_mapped, h.Arena_reserved, &Memstats.Gc_sys)
	h.bitmap_mapped = n
}

// heapBits provides access to the bitmap bits for a single heap word.
// The methods on heapBits take value receivers so that the compiler
// can more easily inline calls to those methods and registerize the
// struct fields independently.
type HeapBits struct {
	Bitp  *uint8
	Shift uint32
}

// heapBitsForAddr returns the heapBits for the address addr.
// The caller must have already checked that addr is in the range [mheap_.arena_start, mheap_.arena_used).
//
// nosplit because it is used during write barriers and must not be preempted.
//go:nosplit
func HeapBitsForAddr(addr uintptr) HeapBits {
	// 2 bits per work, 4 pairs per byte, and a mask is hard coded.
	off := (addr - Mheap_.Arena_start) / PtrSize
	return HeapBits{(*uint8)(unsafe.Pointer(Mheap_.Arena_start - off/4 - 1)), uint32(off & 3)}
}

// heapBitsForObject returns the base address for the heap object
// containing the address p, along with the heapBits for base.
// If p does not point into a heap object,
// return base == 0
// otherwise return the base of the object.
func HeapBitsForObject(p uintptr) (base uintptr, hbits HeapBits, s *Mspan) {
	arenaStart := Mheap_.Arena_start
	if p < arenaStart || p >= Mheap_.Arena_used {
		return
	}
	off := p - arenaStart
	idx := off >> XPageShift
	// p points into the heap, but possibly to the middle of an object.
	// Consult the span table to find the block beginning.
	k := p >> XPageShift
	s = H_spans[idx]
	if s == nil || pageID(k) < s.Start || p >= s.Limit || s.State != MSpanInUse {
		if s == nil || s.State == MSpanStack {
			// If s is nil, the virtual address has never been part of the heap.
			// This pointer may be to some mmap'd region, so we allow it.
			// Pointers into stacks are also ok, the runtime manages these explicitly.
			return
		}

		// The following ensures that we are rigorous about what data
		// structures hold valid pointers.
		// TODO(rsc): Check if this still happens.
		if false {
			// Still happens sometimes. We don't know why.
			Printlock()
			print("runtime:objectstart Span weird: p=", Hex(p), " k=", Hex(k))
			if s == nil {
				print(" s=nil\n")
			} else {
				print(" s.start=", Hex(s.Start<<XPageShift), " s.limit=", Hex(s.Limit), " s.state=", s.State, "\n")
			}
			Printunlock()
			Throw("objectstart: bad pointer in unexpected span")
		}
		return
	}
	// If this span holds object of a power of 2 size, just mask off the bits to
	// the interior of the object. Otherwise use the size to get the base.
	if s.BaseMask != 0 {
		// optimize for power of 2 sized objects.
		base = s.Base()
		base = base + (p-base)&s.BaseMask
		// base = p & s.baseMask is faster for small spans,
		// but doesn't work for large spans.
		// Overall, it's faster to use the more general computation above.
	} else {
		base = s.Base()
		if p-base >= s.Elemsize {
			// n := (p - base) / s.elemsize, using division by multiplication
			n := uintptr(uint64(p-base) >> s.DivShift * uint64(s.DivMul) >> s.DivShift2)
			base += n * s.Elemsize
		}
	}
	// Now that we know the actual base, compute heapBits to return to caller.
	hbits = HeapBitsForAddr(base)
	return
}

// prefetch the bits.
func (h HeapBits) prefetch() {
	Prefetchnta(uintptr(unsafe.Pointer((h.Bitp))))
}

// next returns the heapBits describing the next pointer-sized word in memory.
// That is, if h describes address p, h.next() describes p+ptrSize.
// Note that next does not modify h. The caller must record the result.
func (h HeapBits) Next() HeapBits {
	if h.Shift < 3*HeapBitsShift {
		return HeapBits{h.Bitp, h.Shift + HeapBitsShift}
	}
	return HeapBits{Subtract1(h.Bitp), 0}
}

// forward returns the heapBits describing n pointer-sized words ahead of h in memory.
// That is, if h describes address p, h.forward(n) describes p+n*ptrSize.
// h.forward(1) is equivalent to h.next(), just slower.
// Note that forward does not modify h. The caller must record the result.
// bits returns the heap bits for the current word.
func (h HeapBits) forward(n uintptr) HeapBits {
	n += uintptr(h.Shift) / HeapBitsShift
	return HeapBits{Subtractb(h.Bitp, n/4), uint32(n%4) * HeapBitsShift}
}

// The caller can test isMarked and isPointer by &-ing with bitMarked and bitPointer.
// The result includes in its higher bits the bits for subsequent words
// described by the same bitmap byte.
func (h HeapBits) bits() uint32 {
	return uint32(*h.Bitp) >> h.Shift
}

// isMarked reports whether the heap bits have the marked bit set.
// h must describe the initial word of the object.
func (h HeapBits) IsMarked() bool {
	return *h.Bitp&(BitMarked<<h.Shift) != 0
}

// setMarked sets the marked bit in the heap bits, atomically.
// h must describe the initial word of the object.
func (h HeapBits) SetMarked() {
	// Each byte of GC bitmap holds info for four words.
	// Might be racing with other updates, so use atomic update always.
	// We used to be clever here and use a non-atomic update in certain
	// cases, but it's not worth the risk.
	Atomicor8(h.Bitp, BitMarked<<h.Shift)
}

// setMarkedNonAtomic sets the marked bit in the heap bits, non-atomically.
// h must describe the initial word of the object.
func (h HeapBits) SetMarkedNonAtomic() {
	*h.Bitp |= BitMarked << h.Shift
}

// isPointer reports whether the heap bits describe a pointer word.
// h must describe the initial word of the object.
func (h HeapBits) IsPointer() bool {
	return (*h.Bitp>>h.Shift)&BitPointer != 0
}

// hasPointers reports whether the given object has any pointers.
// It must be told how large the object at h is, so that it does not read too
// far into the bitmap.
// h must describe the initial word of the object.
func (h HeapBits) hasPointers(size uintptr) bool {
	if size == PtrSize { // 1-word objects are always pointers
		return true
	}
	// Otherwise, at least a 2-word object, and at least 2-word aligned,
	// so h.shift is either 0 or 4, so we know we can get the bits for the
	// first two words out of *h.bitp.
	// If either of the first two words is a pointer, not pointer free.
	b := uint32(*h.Bitp >> h.Shift)
	if b&(BitPointer|BitPointer<<HeapBitsShift) != 0 {
		return true
	}
	if size == 2*PtrSize {
		return false
	}
	// At least a 4-word object. Check scan bit (aka marked bit) in third word.
	if h.Shift == 0 {
		return b&(BitMarked<<(2*HeapBitsShift)) != 0
	}
	return uint32(*Subtract1(h.Bitp))&BitMarked != 0
}

// isCheckmarked reports whether the heap bits have the checkmarked bit set.
// It must be told how large the object at h is, because the encoding of the
// checkmark bit varies by size.
// h must describe the initial word of the object.
func (h HeapBits) isCheckmarked(size uintptr) bool {
	if size == PtrSize {
		return (*h.Bitp>>h.Shift)&BitPointer != 0
	}
	// All multiword objects are 2-word aligned,
	// so we know that the initial word's 2-bit pair
	// and the second word's 2-bit pair are in the
	// same heap bitmap byte, *h.bitp.
	return (*h.Bitp>>(HeapBitsShift+h.Shift))&BitMarked != 0
}

// setCheckmarked sets the checkmarked bit.
// It must be told how large the object at h is, because the encoding of the
// checkmark bit varies by size.
// h must describe the initial word of the object.
func (h HeapBits) setCheckmarked(size uintptr) {
	if size == PtrSize {
		Atomicor8(h.Bitp, BitPointer<<h.Shift)
		return
	}
	Atomicor8(h.Bitp, BitMarked<<(HeapBitsShift+h.Shift))
}

// The methods operating on spans all require that h has been returned
// by heapBitsForSpan and that size, n, total are the span layout description
// returned by the mspan's layout method.
// If total > size*n, it means that there is extra leftover memory in the span,
// usually due to rounding.
//
// TODO(rsc): Perhaps introduce a different heapBitsSpan type.

// initSpan initializes the heap bitmap for a span.
func (h HeapBits) InitSpan(size, n, total uintptr) {
	if total%HeapBitmapScale != 0 {
		Throw("initSpan: unaligned length")
	}
	nbyte := total / HeapBitmapScale
	if PtrSize == 8 && size == PtrSize {
		end := h.Bitp
		bitp := Subtractb(end, nbyte-1)
		for {
			*bitp = BitPointerAll
			if bitp == end {
				break
			}
			bitp = Add1(bitp)
		}
		return
	}
	Memclr(unsafe.Pointer(Subtractb(h.Bitp, nbyte-1)), nbyte)
}

// initCheckmarkSpan initializes a span for being checkmarked.
// It clears the checkmark bits, which are set to 1 in normal operation.
func (h HeapBits) InitCheckmarkSpan(size, n, total uintptr) {
	// The ptrSize == 8 is a compile-time constant false on 32-bit and eliminates this code entirely.
	if PtrSize == 8 && size == PtrSize {
		// Checkmark bit is type bit, bottom bit of every 2-bit entry.
		// Only possible on 64-bit system, since minimum size is 8.
		// Must clear type bit (checkmark bit) of every word.
		// The type bit is the lower of every two-bit pair.
		bitp := h.Bitp
		for i := uintptr(0); i < n; i += 4 {
			*bitp &^= BitPointerAll
			bitp = Subtract1(bitp)
		}
		return
	}
	for i := uintptr(0); i < n; i++ {
		*h.Bitp &^= BitMarked << (HeapBitsShift + h.Shift)
		h = h.forward(size / PtrSize)
	}
}

// clearCheckmarkSpan undoes all the checkmarking in a span.
// The actual checkmark bits are ignored, so the only work to do
// is to fix the pointer bits. (Pointer bits are ignored by scanobject
// but consulted by typedmemmove.)
func (h HeapBits) ClearCheckmarkSpan(size, n, total uintptr) {
	// The ptrSize == 8 is a compile-time constant false on 32-bit and eliminates this code entirely.
	if PtrSize == 8 && size == PtrSize {
		// Checkmark bit is type bit, bottom bit of every 2-bit entry.
		// Only possible on 64-bit system, since minimum size is 8.
		// Must clear type bit (checkmark bit) of every word.
		// The type bit is the lower of every two-bit pair.
		bitp := h.Bitp
		for i := uintptr(0); i < n; i += 4 {
			*bitp |= BitPointerAll
			bitp = Subtract1(bitp)
		}
	}
}
