// Copyright 2019 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package runtime

import (
	"math/bits"
)

// pageBits is a bitmap representing one bit per page in an arena.
type pageBits [pagesPerArena / 64]uint64

// set1 clears a single bit in the pageBits at i.
func (b *pageBits) set1(i int) {
	b[i/64] |= 1 << (i % 64)
}

// setRange sets bits in the range [i, i+n).
func (b *pageBits) setRange(i, n int) {
	_ = b[i/64]
	if n == 1 {
		// Add a fast path for the n == 1 case.
		b.set1(i)
		return
	}
	j := i + n - 1
	if i/64 == j/64 {
		b[i/64] = setConsecBits64(b[i/64], i%64, n)
		return
	}
	_ = b[j/64]
	b[i/64] = setConsecBits64(b[i/64], i%64, 64-i%64)
	for k := i/64 + 1; k < j/64; k++ {
		b[k] = ^uint64(0)
	}
	b[j/64] = setConsecBits64(b[j/64], 0, j%64+1)
}

// setAll sets all the bits of b.
func (b *pageBits) setAll() {
	for i := 0; i < len(b); i++ {
		b[i] = ^uint64(0)
	}
}

// clear1 clears a single bit in the pageBits at i.
func (b *pageBits) clear1(i int) {
	b[i/64] &^= 1 << (i % 64)
}

// clearRange clears bits in the range [i, i+n).
func (b *pageBits) clearRange(i, n int) {
	_ = b[i/64]
	if n == 1 {
		// Add a fast path for the n == 1 case.
		b.clear1(i)
		return
	}
	j := i + n - 1
	if i/64 == j/64 {
		b[i/64] = clearConsecBits64(b[i/64], i%64, n)
		return
	}
	_ = b[j/64]
	b[i/64] = clearConsecBits64(b[i/64], i%64, 64-i%64)
	for k := i/64 + 1; k < j/64; k++ {
		b[k] = 0
	}
	b[j/64] = clearConsecBits64(b[j/64], 0, j%64+1)
}

// clearAll frees all the bits of b.
func (b *pageBits) clearAll() {
	for i := range b {
		b[i] = 0
	}
}

// popcntRange counts the number of set bits in the
// range [i, i+n).
func (b *pageBits) popcntRange(i, n int) (s int) {
	if n == 1 {
		return int((b[i/64] >> (i % 64)) & 1)
	}
	// TODO: make this faster with wider aligned loads.
	_ = b[i/64]
	j := i + n - 1
	if i/64 == j/64 {
		return bits.OnesCount64((b[i/64] >> (i % 64)) & ((1 << n) - 1))
	}
	_ = b[j/64]
	s += bits.OnesCount64(b[i/64] >> (i % 64))
	for k := i/64 + 1; k < j/64; k++ {
		s += bits.OnesCount64(b[k])
	}
	s += bits.OnesCount64(b[j/64] & ((1 << (j%64 + 1)) - 1))
	return
}

// mallocBits is a page-per-bit bitmap.
//
// It wraps a pageBits with different names and additional features,
// such as summarizing and searching.
//
// TODO(mknyszek): Consider making all of mallocBits' methods accept
// a chunk index and have the searchIdx be relative to the chunk. This way,
// we avoid situations where the page allocator succeeds where it should
// have failed, making bugs more difficult to identify.
type mallocBits pageBits

// consec8tab is a table containing the number of consecutive
// zero bits for any uint8 value.
//
// The table is generated by calling consec8(i) for each
// possible uint8 value, which is defined as:
//
// // consec8 counts the maximum number of consecutive 0 bits
// // in a uint8.
// func consec8(n uint8) int {
// 	n = ^n
// 	i := 0
// 	for n != 0 {
// 		n &= (n << 1)
// 		i++
// 	}
// 	return i
// }
var consec8tab = [256]int{
	8, 7, 6, 6, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,
	4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,
	5, 4, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2,
	4, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2,
	6, 5, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2,
	4, 3, 2, 2, 2, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1,
	5, 4, 3, 3, 2, 2, 2, 2, 3, 2, 1, 1, 2, 1, 1, 1,
	4, 3, 2, 2, 2, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1,
	7, 6, 5, 5, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3,
	4, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2,
	5, 4, 3, 3, 2, 2, 2, 2, 3, 2, 1, 1, 2, 1, 1, 1,
	4, 3, 2, 2, 2, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1,
	6, 5, 4, 4, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2,
	4, 3, 2, 2, 2, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 1,
	5, 4, 3, 3, 2, 2, 2, 2, 3, 2, 1, 1, 2, 1, 1, 1,
	4, 3, 2, 2, 2, 1, 1, 1, 3, 2, 1, 1, 2, 1, 1, 0,
}

// summarize returns a packed summary of the n'th chunk of the bitmap
// in mallocBits.
//
// TODO(mknyszek): There may be something more clever to be done
// here to make the summarize operation more efficient. For example,
// we can compute start and end with 64-bit wide operations easily,
// but max is a bit more complex. Perhaps there exists some way to
// leverage the 64-bit start and end to our advantage?
func (b *mallocBits) summarize(n int) mallocSum {
	bot := n * mallocChunkPages / 64
	top := (n + 1) * mallocChunkPages / 64
	s := b[bot:top] // extract the chunk

	start, max, end := 0, 0, 0
	for i := 0; i < len(s); i++ {
		a := s[i]
		for j := 0; j < 64; j += 8 {
			k := uint8(a >> j)

			// Compute start.
			si := bits.TrailingZeros8(k)
			if start == i*64+j {
				start += si
			}

			// Compute max.
			if end+si > max {
				max = end + si
			}
			if mi := consec8tab[k]; mi > max {
				max = mi
			}

			// Compute end.
			if k == 0 {
				end += 8
			} else {
				end = bits.LeadingZeros8(k)
			}
		}
	}
	return packMallocSum(start, max, end)
}

// alloc allocates npages mallocBits from this mallocBits and returns
// the index where that run of contiguous mallocBits starts as well as a
// new searchIdx.
//
// If alloc fails to find any free space, it returns an index of -1 and
// the new searchIdx should be ignored.
//
// The returned searchIdx is always the index of the first free page found
// in this bitmap during the search, except if npages == 1, in which
// case it will be the index after the first free page, because that
// index is assumed to be allocated and so represents a minor
// optimization for that case.
//
// searchIdx represents the first known index and where to begin
// the search from.
func (b *mallocBits) alloc(npages uintptr, searchIdx int) (int, int) {
	if npages == 1 {
		addr := b.alloc1(searchIdx)
		// Return a searchIdx of addr + 1 since we assume addr will be
		// allocated.
		return addr, addr + 1
	} else if npages <= 64 {
		return b.allocSmallN(npages, searchIdx)
	}
	return b.allocLargeN(npages, searchIdx)
}

// alloc1 is a helper for alloc which allocates a single page from the mallocBits
// and returns the index.
//
// See alloc for an explanation of the searchIdx parameter.
func (b *mallocBits) alloc1(searchIdx int) int {
	for i := searchIdx / 64; i < len(b); i++ {
		x := b[i]
		if x == ^uint64(0) {
			continue
		}
		z := bits.TrailingZeros64(^x)
		// z will always be in the range [0, 63) since we
		// skipped the one case where z == 64 earlier.
		b[i] |= 1 << z
		return i*64 + z
	}
	return -1
}

// allocSmallN is a helper for alloc which allocates npages mallocBits from
// this mallocBits and returns the index where that run of contiguous mallocBits
// starts as well as a new searchIdx. See alloc for an explanation of the searchIdx parameter.
//
// Returns a -1 index on failure and the new searchIdx should be ignored.
//
// allocSmallN assumes npages <= 64, where any such allocation
// crosses at most one aligned 64-bit chunk boundary in the bits.
func (b *mallocBits) allocSmallN(npages uintptr, searchIdx int) (int, int) {
	end, nSearchIdx := int(0), -1
	for i := searchIdx / 64; i < len(b); i++ {
		bi := b[i]
		if bi == ^uint64(0) {
			end = 0
			continue
		}
		// First see if we can pack our allocation in the trailing
		// zeros plus the end of the last 64 bits.
		start := bits.TrailingZeros64(bi)
		if nSearchIdx == -1 {
			// The new searchIdx is going to be at these 64 bits after any
			// 1s we file, so count trailing 1s.
			nSearchIdx = i*64 + bits.TrailingZeros64(^bi)
		}
		if end+start >= int(npages) {
			if end != 0 {
				// Set the end highest mallocBits and store.
				b[i-1] = setConsecBits64(b[i-1], 64-end, end)
			}
			// Set the npages-end lowest mallocBits and store.
			b[i] = setConsecBits64(bi, 0, int(npages)-end)
			return i*64 - end, nSearchIdx
		}
		// Next, check the interior of the 64-bit chunk.
		j := findConsecN64(^bi, int(npages))
		if j < 64 {
			// Set mallocBits [j, j+npages) and store.
			b[i] = setConsecBits64(bi, j, int(npages))
			return i*64 + j, nSearchIdx
		}
		end = bits.LeadingZeros64(bi)
	}
	return -1, nSearchIdx
}

// allocLargeN is a helper for alloc which allocates npages mallocBits from
// this mallocBits and returns the index where that run of contiguous mallocBits
// starts as well as a new searchIdx. See alloc for an explanation of the searchIdx parameter.
//
// Returns a -1 index on failure and the new searchIdx should be ignored.
//
// allocLargeN assumes npages > 64, where any such allocation
// crosses at least one aligned 64-bit chunk boundary in the bits.
func (b *mallocBits) allocLargeN(npages uintptr, searchIdx int) (int, int) {
	start, size, nSearchIdx := -1, int(0), -1
	for i := searchIdx / 64; i < len(b); i++ {
		x := b[i]
		if x == ^uint64(0) {
			size = 0
			continue
		}
		if nSearchIdx == -1 {
			// The new searchIdx is going to be at these 64 bits after any
			// 1s we file, so count trailing 1s.
			nSearchIdx = i*64 + bits.TrailingZeros64(^x)
		}
		if size == 0 {
			size = bits.LeadingZeros64(x)
			start = i*64 + 64 - size
			continue
		}
		s := bits.TrailingZeros64(x)
		if s+size >= int(npages) {
			size += s
			break
		}
		if s < 64 {
			size = bits.LeadingZeros64(x)
			start = i*64 + 64 - size
			continue
		}
		size += 64
	}
	if size < int(npages) {
		return -1, nSearchIdx
	}
	b.allocRange(start, int(npages))
	return start, nSearchIdx
}

// allocRange allocates the range [i, i+n).
func (b *mallocBits) allocRange(i, n int) {
	(*pageBits)(b).setRange(i, n)
}

// allocAll allocates all the bits of b.
func (b *mallocBits) allocAll() {
	(*pageBits)(b).setAll()
}

// free1 frees a single page in the mallocBits at i.
func (b *mallocBits) free1(i int) {
	(*pageBits)(b).clear1(i)
}

// free frees the range [i, i+n) of pages in the mallocBits.
func (b *mallocBits) free(i, n int) {
	(*pageBits)(b).clearRange(i, n)
}

// freeAll frees all the bits of b.
func (b *mallocBits) freeAll() {
	(*pageBits)(b).clearAll()
}

// setConsecBits64 sets n consecutive bits to 1 in x starting
// at bit index i.
func setConsecBits64(x uint64, i, n int) uint64 {
	return x | ((uint64(1<<n) - 1) << i)
}

// clearConsecBits64 sets n consecutive bits to 0 in x starting
// at bit index i.
func clearConsecBits64(x uint64, i, n int) uint64 {
	return x &^ ((uint64(1<<n) - 1) << i)
}

// findConsecN64 returns the bit index of the first set of
// n consecutive 1 bits. If no consecutive set of 1 bits of
// size n may be found in c, then it returns an integer > 64.
func findConsecN64(c uint64, n int) int {
	i := 0
	cont := bits.TrailingZeros64(^c)
	for cont < n && i < 64 {
		i += cont
		i += bits.TrailingZeros64(c >> i)
		cont = bits.TrailingZeros64(^(c >> i))
	}
	return i
}

// mallocData encapsulates mallocBits and a bitmap for
// whether or not a given page is scavenged in a single
// structure. It's effectively a mallocBits with
// additional functionality.
type mallocData struct {
	mallocBits
	scavenged pageBits
}

// alloc allocates npages bits starting from the given searchIdx.
//
// Updates scavenged metadata appropriately.
//
// Returns a page index (indicating the base of the allocation),
// a new searchIdx, and the number of pages that were scavenged in
// the newly-allocated region.
func (m *mallocData) alloc(npages uintptr, searchIdx int) (int, int, int) {
	b, nSearchIdx := m.mallocBits.alloc(npages, searchIdx)
	scav := m.scavenged.popcntRange(b, int(npages))
	// Clear the scavenged bits when we alloc.
	if npages == 1 {
		m.scavenged.clear1(b)
	} else {
		m.scavenged.clearRange(b, int(npages))
	}
	return b, nSearchIdx, scav
}

// allocRange sets bits [i, i+n) in the bitmap to 1 and
// updates the scavenged bits appropriately. Returns the number
// of scavenged bits in the range prior to updating scavenged
// bits.
func (m *mallocData) allocRange(i, n int) int {
	m.mallocBits.allocRange(i, n)
	scav := m.scavenged.popcntRange(i, n)
	// Clear the scavenged bits when we alloc the range.
	m.scavenged.clearRange(i, n)
	return scav
}

// allocAll sets every bit in the bitmap to 1 and updates
// the scavenged bits appropriately. Returns the number of
// scavenged bits in the range prior to updating scavenged
// bits.
func (m *mallocData) allocAll() int {
	m.mallocBits.allocAll()
	scav := m.scavenged.popcntRange(0, pagesPerArena)
	// Clear the scavenged bits when we alloc the range.
	m.scavenged.clearAll()
	return scav
}
