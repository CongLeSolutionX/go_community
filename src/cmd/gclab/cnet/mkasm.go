// Copyright 2024 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

//go:build ignore

package main

import (
	"cmp"
	"encoding/binary"
	"fmt"
	"hash/maphash"
	"io"
	"iter"
	"log"
	"maps"
	"math/bits"
	"os"
	"reflect"
	"slices"
	"strings"
)

// TODO: This file is way too big. Put it in a package so it can have multiple
// source files.

// logCompile, if set, logs compilation steps.
const logCompile = false

var class_to_size = [...]uint16{0, 8, 16, 24, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 576, 640, 704, 768, 896, 1024, 1152, 1280, 1408, 1536, 1792, 2048, 2304, 2688, 3072, 3200, 3456, 4096, 4864, 5376, 6144, 6528, 6784, 6912, 8192, 9472, 9728, 10240, 10880, 12288, 13568, 14336, 16384, 18432, 19072, 20480, 21760, 24576, 27264, 28672, 32768}
var class_to_allocnpages = [...]uint8{0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 2, 3, 1, 3, 2, 3, 4, 5, 6, 1, 7, 6, 5, 4, 3, 5, 7, 2, 9, 7, 5, 8, 3, 10, 7, 4}

const ptrSize = 8
const ptrBits = 8 * ptrSize
const minSizeForMallocHeader = ptrSize * ptrBits

func main() {
	// TODO: Restructure this so it can generate multiple assembly files.

	outf := os.Stdout
	defer outf.Close()

	fmt.Fprintf(outf, `// Code generated by mkasm.go. DO NOT EDIT.

#include "go_asm.h"
#include "textflag.h"

`)

	if false {
		// Annoying vector constants for some experiments.
		var byteIndexes [128]uint8
		for i := range byteIndexes {
			byteIndexes[i] = uint8(i)
		}
		emitConst(outf, "byteIndexes<>", byteIndexes)

		var shiftR1 [64]uint8
		for i := range shiftR1 {
			shiftR1[i] = uint8(i + 1)
		}
		emitConst(outf, "shiftR1<>", shiftR1)

		var frameStarts [512 / 32]uint32
		for i := range frameStarts {
			frameStarts[i] = uint32(i * 64)
		}
		emitConst(outf, "frameStarts<>", frameStarts)
	}

	var funcs []*Func
	sizeClassTab := make([]*Func, len(class_to_size))
	for sc, ob := range class_to_size {
		if class_to_allocnpages[sc] != 1 {
			// These functions all produce a bitmap that covers exactly one
			// page.
			continue
		}
		if ob > minSizeForMallocHeader {
			// This size class is too big to have a packed pointer/scalar bitmap.
			break
		}
		if ob == 8 {
			// TODO: Handle this case. Just pass through.
			continue
		}
		xf := int(ob) / 8
		log.Printf("size class %d bytes, expansion %dx", ob, xf)
		f := gfExpander(xf)
		if f == nil {
			// TODO: Temporary while gfExpander sometimes fails
			continue
		}
		funcs = append(funcs, f)
		sizeClassTab[sc] = f
	}

	// Generate table mapping size class to expander PC
	emitFuncTab(outf, "Â·gcExpanders", sizeClassTab)

	compile(outf, funcs)
}

func fatalf(f string, args ...any) {
	panic(fmt.Sprintf(f, args...))
}

// mat8x8 is an 8x8 bit matrix.
type mat8x8 struct {
	mat [8]uint8
}

func matGroupToVec(mats *[8]mat8x8) [8]uint64 {
	var out [8]uint64
	for i, mat := range mats {
		for j, row := range mat.mat {
			// For some reason, Intel flips the rows.
			out[i] |= uint64(row) << ((7 - j) * 8)
		}
	}
	return out
}

// gfExpander produces a function that expands each bit in an input bitmap into
// f consecutive bits in an output bitmap.
//
// The input is
//
//	AX *[64]uint8 = A pointer to floor(1024/f) bits
//
// The output is
//
//	Z1 [64]uint8  = The bottom 512 bits of the expanded bitmap
//	Z2 [64]uint8  = The top 512 bits of the expanded bitmap
func gfExpander(f int) *Func {
	// TODO: For powers of 2 >= 8, we can use mask expansion ops to make this much simpler.

	// TODO: For f >= 8, I suspect there are better ways to do this.
	//
	// For example, we could use a mask expansion to get a full byte for each
	// input bit, and separately create the bytes that blend adjacent bits, then
	// shuffle those bytes together. Certainly for f >= 16 this makes sense
	// because each of those bytes will be used, possibly more than once.

	if f >= 12 {
		// TODO: Code gen fails right now for most of these because the shuffle
		// has too many inputs. Sit down and think about whether this approach
		// is actually right at all for these larger factors.
		return nil
	}

	//packedOp := opDeref(&op{op: "arg", typ: "*[64]uint8", c: 0, name: "packed"})

	packedOp := opDeref(&op{op: "argReg", typ: "*[64]uint8", c: locReg{cls: regClassGP, reg: 0}})

	type term struct {
		iByte, oByte int
		mat          mat8x8
	}
	var terms []term

	// Iterate over all output bytes and construct the 8x8 GF2 matrix to compute
	// the output byte from the appropriate input byte. Gather all of these into
	// "terms".
	for oByte := 0; oByte < 1024/8; oByte++ {
		var byteMat mat8x8
		iByte := -1
		for oBit := oByte * 8; oBit < oByte*8+8; oBit++ {
			iBit := oBit / f
			if iByte == -1 {
				iByte = iBit / 8
			} else if iByte != iBit/8 {
				fatalf("output byte %d straddles input bytes %d and %d", oByte, iByte, iBit/8)
			}
			// One way to view this is that the i'th row of the matrix will be
			// ANDed with the input byte, and the parity of the result will set
			// the i'th bit in the output. We use a simple 1 bit mask, so the
			// parity is irrelevant beyond selecting out that one bit.
			byteMat.mat[oBit%8] = 1 << (iBit % 8)
		}
		terms = append(terms, term{iByte, oByte, byteMat})
	}

	if false {
		// Print input byte -> output byte as a matrix
		maxIByte, maxOByte := 0, 0
		for _, term := range terms {
			maxIByte = max(maxIByte, term.iByte)
			maxOByte = max(maxOByte, term.oByte)
		}
		iToO := make([][]rune, maxIByte+1)
		for i := range iToO {
			iToO[i] = make([]rune, maxOByte+1)
		}
		matMap := make(map[mat8x8]int)
		for _, term := range terms {
			i, ok := matMap[term.mat]
			if !ok {
				i = len(matMap)
				matMap[term.mat] = i
			}
			iToO[term.iByte][term.oByte] = 'A' + rune(i)
		}
		for o := range maxOByte + 1 {
			fmt.Printf("%d", o)
			for i := range maxIByte + 1 {
				fmt.Printf(",")
				if mat := iToO[i][o]; mat != 0 {
					fmt.Printf("%c", mat)
				}
			}
			fmt.Println()
		}
	}

	// In hardware, each (8 byte) matrix applies to 8 bytes of data in parallel,
	// and we get to operate on up to 8 matrixes in parallel (or 64 values). That is:
	//
	//  abcdefgh ijklmnop qrstuvwx yzABCDEF GHIJKLMN OPQRSTUV WXYZ0123 456789_+
	//    mat0     mat1     mat2     mat3     mat4     mat5     mat6     mat7

	// Group the terms by matrix, but limit each group to 8 terms.
	const termsPerGroup = 8       // Number of terms we can multiply by the same matrix.
	const groupsPerSuperGroup = 8 // Number of matrixes we can fit in a vector.

	matMap := make(map[mat8x8]int)
	allMats := make(map[mat8x8]bool)
	var termGroups [][]term
	for _, term := range terms {
		allMats[term.mat] = true

		i, ok := matMap[term.mat]
		if ok && f > groupsPerSuperGroup {
			// The output is ultimately produced in two [64]uint8 registers.
			// Getting every byte in the right place of each of these requires a
			// final permutation that often requires more than one source.
			//
			// Up to 8x expansion, we can get really nice grouping so we can use
			// the same 8 matrix vector several times, without producing
			// permutations that require more than two sources.
			//
			// Above 8x, however, we can't get nice matrixes anyway, so we
			// instead prefer reducing the complexity of the permutations we
			// need to produce the final outputs. To do this, avoid grouping
			// together terms that are split across the two registers.
			outRegister := termGroups[i][0].oByte / 64
			if term.oByte/64 != outRegister {
				ok = false
			}
		}
		if !ok {
			// Start a new term group.
			i = len(termGroups)
			matMap[term.mat] = i
			termGroups = append(termGroups, nil)
		}

		termGroups[i] = append(termGroups[i], term)

		if len(termGroups[i]) == termsPerGroup {
			// This term group is full.
			delete(matMap, term.mat)
		}
	}

	for i, termGroup := range termGroups {
		log.Printf("term group %d:", i)
		for _, term := range termGroup {
			log.Printf("  %+v", term)
		}
	}

	// We can do 8 matrix multiplies in parallel, which is 8 term groups. Pack
	// as many term groups as we can into each super-group to minimize the
	// number of matrix multiplies.
	//
	// Ideally, we use the same matrix in each super-group, which might mean
	// doing fewer than 8 multiplies at a time. That's fine because it never
	// increases the total number of matrix multiplies.
	//
	// TODO: Packing the matrixes less densely may let us use more broadcast
	// loads instead of general permutations, though. That replaces a load of
	// the permutation with a load of the matrix, but is probably still slightly
	// better.
	var sgSize, nSuperGroups int
	oneMatVec := f <= groupsPerSuperGroup
	if oneMatVec {
		// We can use the same matrix in each multiply by doing sgSize
		// multiplies at a time.
		sgSize = groupsPerSuperGroup / len(allMats) * len(allMats)
		nSuperGroups = (len(termGroups) + sgSize - 1) / sgSize
	} else {
		// We can't use the same matrix for each multiply. Just do as many at a
		// time as we can.
		//
		// TODO: This is going to produce several distinct matrixes, when we
		// probably only need two. Be smarter about how we create super-groups
		// in this case. Maybe we build up an array of super-groups and then the
		// loop below just turns them into ops?
		sgSize = 8
		nSuperGroups = (len(termGroups) + groupsPerSuperGroup - 1) / groupsPerSuperGroup
	}

	// Construct each super-group.
	var matGroup [8]mat8x8
	var matMuls []*op
	var perm [128]int
	for sgi := range nSuperGroups {
		var iperm [64]int
		for i := range iperm {
			iperm[i] = -1 // "Don't care"
		}
		// Pick off sgSize term groups.
		superGroup := termGroups[:min(len(termGroups), sgSize)]
		termGroups = termGroups[len(superGroup):]
		// Build the matrix and permutations for this super-group.
		var thisMatGroup [8]mat8x8
		for i, termGroup := range superGroup {
			// All terms in this group have the same matrix. Pick one.
			thisMatGroup[i] = termGroup[0].mat
			for j, term := range termGroup {
				// Build the input permutation.
				iperm[i*termsPerGroup+j] = term.iByte
				// Build the output permutation.
				perm[term.oByte] = sgi*groupsPerSuperGroup*termsPerGroup + i*termsPerGroup + j
			}
		}
		log.Printf("input permutation %d: %v", sgi, iperm)

		// Check that we're not making more distinct matrixes than expected.
		if oneMatVec {
			if sgi == 0 {
				matGroup = thisMatGroup
			} else if matGroup != thisMatGroup {
				fatalf("super-groups have different matrixes:\n%+v\n%+v", matGroup, thisMatGroup)
			}
		}

		// Emit matrix op.
		matConst := opConst(thisMatGroup, fmt.Sprintf("gfExpand%d_%d<>", f, sgi))
		inOp := opShuffle(true, &iperm, packedOp)
		matMulOp := &op{op: "VGF2P8AFFINEQB", typ: "[64]uint8", args: []*op{matConst, inOp}}
		matMuls = append(matMuls, matMulOp)
	}

	log.Printf("output permutation: %v", perm)

	outLo := opShuffle(true, (*[64]int)(perm[:64]), matMuls...)
	outHi := opShuffle(f < 14, (*[64]int)(perm[64:]), matMuls...)
	out := &op{op: "return", args: []*op{outLo, outHi}}

	return &Func{fmt.Sprintf("expand%d<>", f), out}
}

type Func struct {
	name string
	root *op
}

type op struct {
	op   string
	typ  string
	args []*op

	c    any // "const" value
	name string
}

func opConst(val any, name string) *op {
	typ := reflect.TypeOf(val).String()
	return &op{op: "const", typ: typ, c: val, name: name}
}

func opDeref(arg *op) *op {
	if arg.typ[0] != '*' {
		panic("not a pointer")
	}
	resTyp := arg.typ[1:]
	return &op{op: "deref", typ: resTyp, args: []*op{arg}}
}

// opShuffle logically concatenates args, and then selects bytes specified by shuf.
func opShuffle(simple bool, shuf *[64]int, args ...*op) *op {
	for _, arg := range args {
		if arg.typ != "[64]uint8" {
			fatalf("input must be [64]uint8, not %s", arg.typ)
		}
	}
	// Range check the shuffle. Also, with a simple shuffle, we can select from
	// at most two of the inputs, so check that, too. (With fancier lowering we
	// could do more, of course.)
	usedArgs := make(map[*op][]int)
	for _, inp := range shuf {
		// -1 is a special "don't care" value
		if inp < -1 || inp >= 64*len(args) {
			fatalf("shuffle source %d out of range [0, %d)", inp, 64*len(args))
		}
		inOp := args[inp/64]
		usedArgs[inOp] = append(usedArgs[inOp], inp)
	}
	// TODO: We haven't yet implemented support for non-simple shuffles.
	simple = true
	if simple && len(usedArgs) > 2 {
		var errBuf strings.Builder
		fmt.Fprintf(&errBuf, "%d > 2 inputs used by shuffle %d; groups:", len(usedArgs), shuf[:])
		for _, group := range slices.SortedFunc(maps.Values(usedArgs), func(a, b []int) int {
			return cmp.Or(cmp.Compare(len(a), len(b)), cmp.Compare(a[0], b[0]))
		}) {
			fmt.Fprintf(&errBuf, "\n    %d", group)
		}
		fatalf("%s", &errBuf)
	}
	return &op{op: "shuffle", typ: "[64]uint8", c: *shuf, args: args}
}

func opShuffleLower(o *op) *op {
	// TODO: There are often patterns we can take advantage of here. Sometimes
	// we can do a broadcast. Sometimes we can at least do a quadword
	// permutation instead of a full byte permutation.

	perm := o.c.([64]int)
	// Construct VPERMB/VPERMT2B input
	var vperm [64]byte
	// Get the two inputs used by this permutation (we already checked there
	// were at most two).
	var inputs []int
	for i, src := range perm {
		inputIdx := slices.Index(inputs, src/64)
		if inputIdx == -1 {
			inputIdx = len(inputs)
			inputs = append(inputs, src/64)
		}
		vperm[i] = byte(src%64 | (inputIdx << 6))
	}
	// Emit instruction.
	constOp := opConst(vperm, "")
	if len(inputs) == 1 {
		return &op{op: "VPERMB", args: []*op{o.args[inputs[0]], constOp}}
	} else if len(inputs) == 2 {
		// Confusingly, the inputs are in the opposite order from what you'd expect.
		return &op{op: "VPERMI2B", args: []*op{o.args[inputs[1]], o.args[inputs[0]], constOp}}
	}
	panic("> 2 inputs not implemented")
}

func (o *op) print(w io.Writer) {
	num := make(map[*op]int)
	for o2 := range o.visit() {
		id := len(num)
		num[o2] = id
		fmt.Fprintf(w, "v%02d = %s [%s]", id, o2.op, o2.typ)
		for _, arg := range o2.args {
			fmt.Fprintf(w, " v%02d", num[arg])
		}
		if o2.c != nil {
			fmt.Fprintf(w, " %v", o2.c)
		}
		if o2.name != "" {
			fmt.Fprintf(w, " %q", o2.name)
		}
		fmt.Fprintf(w, "\n")
	}
}

func (o *op) equalNoName(o2 *op) bool {
	if o.op != o2.op || o.typ != o2.typ || o.c != o2.c || len(o.args) != len(o2.args) {
		return false
	}
	for i, arg := range o.args {
		if o2.args[i] != arg {
			return false
		}
	}
	return true
}

func (o *op) visit() iter.Seq[*op] {
	visited := make(map[*op]bool)
	var rec func(*op, func(*op) bool) bool
	rec = func(o *op, yield func(*op) bool) bool {
		if visited[o] {
			return true
		}
		visited[o] = true
		for _, arg := range o.args {
			if !rec(arg, yield) {
				return false
			}
		}
		return yield(o)
	}
	return func(yield func(*op) bool) { rec(o, yield) }
}

func (o *op) visitReplace(fn func(*op) *op) *op {
	visited := make(map[*op]*op)
	return o.visitReplace1(visited, fn)
}

func (o *op) visitReplace1(visited map[*op]*op, fn func(*op) *op) *op {
	if v, ok := visited[o]; ok {
		if v == nil {
			panic("visit cycle")
		}
		return v
	}
	visited[o] = nil

	var nOp *op
	for i, arg := range o.args {
		nArg := arg.visitReplace1(visited, fn)
		if nArg == arg {
			continue
		}
		// Something changed, so we need a new op
		if nOp == nil {
			nOp = &*o
			nOp.args = slices.Clone(o.args)
		}
		nOp.args[i] = nArg
	}
	if nOp == nil {
		nOp = o
	}

	nOp = fn(nOp)
	visited[o] = nOp
	return nOp
}

func compile(f io.Writer, funcs []*Func) {
	for _, fn := range funcs {
		if logCompile {
			log.Printf("## Compiling %s", fn.name)
			fn.root.print(os.Stderr)
		}
		root := lower(fn.root)
		if logCompile {
			log.Printf("## Lowered")
			root.print(os.Stderr)
			log.Printf("")
		}
		emit(f, root, fn.name)
	}
}

func lower(root *op) *op {
	for {
		oldRoot := root
		root = root.visitReplace(lower1)
		root = cse(root)
		if root == oldRoot {
			break
		}
	}

	root = addLoads(root)

	return root
}

func lower1(o *op) *op {
	switch o.op {
	case "const":
		if mat, ok := o.c.([8]mat8x8); ok {
			return &op{op: "const", typ: "[8]uint64", c: matGroupToVec(&mat), name: o.name}
		}
	case "shuffle":
		return opShuffleLower(o)
	}
	return o
}

func cse(root *op) *op {
	// Compute structural hashes
	hashes := make(map[*op]uint64)
	var h maphash.Hash
	var bbuf [8]byte
	for o := range root.visit() {
		// We ignore the name for canonicalization.
		h.Reset()
		h.WriteString(o.op)
		h.WriteString(o.typ)
		// TODO: Ideally we would hash o1.c, but we don't have a good way to do that.
		for _, arg := range o.args {
			binary.NativeEndian.PutUint64(bbuf[:], hashes[arg])
			h.Write(bbuf[:])
		}
		hashes[o] = h.Sum64()
	}

	// Canonicalize ops.
	canon := make(map[uint64][]*op)
	return root.visitReplace(func(o *op) *op {
		hash := hashes[o]
		for _, o2 := range canon[hash] {
			if o.equalNoName(o2) {
				return o2
			}
		}
		canon[hash] = append(canon[hash], o)
		return o
	})
}

// canMem is a map from operation to a bitmap of which arguments can use a
// direct memory reference.
var canMem = map[string]uint64{
	"VPERMB":         1 << 0,
	"VPERMI2B":       1 << 0,
	"VPERMT2B":       1 << 0,
	"VGF2P8AFFINEQB": 1 << 0,
}

func addLoads(root *op) *op {
	// TODO: Having to create a new op every time something changes and
	// carefully NOT create a new op if it doesn't change is a pain.

	// A lot of operations can directly take memory locations. If there's only a
	// single reference to a deref operation, and the operation can do the deref
	// itself, eliminate the deref. If there's more than one reference, then we
	// leave the load so we can share the value in the register.
	nRefs := opRefs(root)
	loads := make(map[*op]*op) // deref -> load
	return root.visitReplace(func(o *op) *op {
		var needCopy uint64
		canMask := canMem[o.op]
		for i, arg := range o.args {
			// These all produce memory locations.
			if arg.op == "deref" || arg.op == "arg" || arg.op == "const" {
				if canMask&(1<<i) == 0 || nRefs[arg] > 1 {
					needCopy |= 1 << i
				}
			}
		}
		if needCopy == 0 {
			return o
		}
		// One or more of the arguments to the op is a memory location, but
		// needs to be loaded into a register. Make load ops for them.
		o2 := &*o
		o2.args = slices.Clone(o.args)
		for i, arg := range o.args {
			if needCopy&(1<<i) == 0 {
				continue
			}
			load, ok := loads[arg]
			if !ok {
				load = makeLoad(arg)
				loads[arg] = load
			}
			o2.args[i] = load
		}
		return o2
	})
}

func opRefs(o *op) map[*op]int {
	refs := make(map[*op]int)
	for o1 := range o.visit() {
		for _, arg := range o1.args {
			refs[arg]++
		}
	}
	refs[o]++
	return refs
}

func makeLoad(deref *op) *op {
	typ := deref.typ
	var inst string
	switch {
	default:
		fatalf("don't know how to load %s", typ)
	case strings.HasPrefix(typ, "*"):
		inst = "MOVQ"
	case typ == "[64]uint8", typ == "[8]uint64":
		inst = "VMOVDQU64"
	}
	return &op{op: inst, typ: typ, args: []*op{deref}}
}

const traceRegAlloc = false

type regClass uint8

const (
	regClassFixed regClass = iota
	regClassGP
	regClassZ

	numRegClasses
)

type locReg struct {
	cls regClass
	reg int
}

func (l locReg) LocString() string {
	switch l.cls {
	case regClassFixed:
		return fixedRegs[l.reg]
	case regClassGP:
		return gpRegs[l.reg]
	case regClassZ:
		return fmt.Sprintf("Z%d", l.reg)
	}
	panic("bad register class")
}

func (l locReg) Deref() (loc, error) {
	return locMem{l, 0, ""}, nil
}

func (l locReg) Reg() (locReg, bool) {
	return l, true
}

type locMem struct {
	base locReg
	off  int
	name string
}

func (l locMem) LocString() string {
	if l.base.cls == regClassFixed && l.base.reg == regSB && l.off == 0 {
		return l.name + "(SB)"
	}
	if l.name != "" {
		return fmt.Sprintf("%s+%d(%s)", l.name, l.off, l.base.LocString())
	}
	if l.off != 0 {
		return fmt.Sprintf("%d(%s)", l.off, l.base.LocString())
	}
	return "(" + l.base.LocString() + ")"
}

func (l locMem) Deref() (loc, error) {
	return nil, fmt.Errorf("cannot dereference already memory address %s", l.LocString())
}

func (l locMem) Reg() (locReg, bool) {
	if l.base.cls == regClassFixed {
		return locReg{}, false
	}
	return l.base, true
}

type loc interface {
	LocString() string   // Return the assembly syntax for this location
	Deref() (loc, error) // Treat this location as an address and return a location with the contents of memory at that address
	Reg() (locReg, bool) // Register used by this location
}

var opRegClass = map[string]regClass{
	"MOVQ":           regClassGP,
	"VMOVDQU64":      regClassZ,
	"VPERMB":         regClassZ,
	"VPERMI2B":       '2', // Overwrites third argument XXX Check
	"VPERMT2B":       '1', // Overwrites second argument XXX Check
	"VGF2P8AFFINEQB": regClassZ,
}

const (
	regSB = iota
	regFP
)

var fixedRegs = []string{regSB: "SB", regFP: "FP"}
var gpRegs = []string{"AX", "BX", "CX", "DX", "SI", "DI"}

type regSet struct {
	inUse [numRegClasses]uint32
}

func (s *regSet) used(l loc) {
	if l == nil {
		return
	}
	reg, ok := l.Reg()
	if !ok {
		return
	}
	if traceRegAlloc {
		log.Printf("  alloc %s", reg.LocString())
	}
	if s.inUse[reg.cls]&(1<<reg.reg) != 0 {
		fatalf("register %s already used", reg.LocString())
	}
	s.inUse[reg.cls] |= 1 << reg.reg
}

func (s *regSet) free(l loc) {
	if l == nil {
		return
	}
	reg, ok := l.Reg()
	if !ok {
		return
	}
	if traceRegAlloc {
		log.Printf("  free %s", reg.LocString())
	}
	if s.inUse[reg.cls]&(1<<reg.reg) == 0 {
		fatalf("register %s is not in use", reg.LocString())
	}
	s.inUse[reg.cls] &^= 1 << reg.reg
}

func assignLocs(root *op, nameBase string) map[*op]loc {
	// Remove static indicator on name, if any. We'll add it back.
	nameBase = strings.TrimSuffix(nameBase, "<>")

	// Gather ops into a list so we have a fixed order to work with and can
	// index into the list.
	var ops []*op
	opIndexes := make(map[*op]int) // op -> index in ops
	for op := range root.visit() {
		opIndexes[op] = len(ops)
		ops = append(ops, op)
	}

	// Read-modify-write operations share a location with one of their inputs.
	// Compute a map from each op to a "canonical" op whose location we'll use.
	canon := make(map[*op]*op)
	for _, o := range ops {
		cls := opRegClass[o.op]
		switch cls {
		case '0', '1', '2':
			canon[o] = canon[o.args[cls-'0']]
		default:
			canon[o] = o
		}
	}

	// Compute live ranges of each canonical op.
	//
	// First, find the last use of each op.
	lastUses := make(map[*op]*op) // Canonical creation op -> last use op
	for _, op := range ops {
		for _, arg := range op.args {
			lastUses[canon[arg]] = op
		}
	}
	// Invert the last uses map to get a map from op to the (canonical) values
	// that die at that op.
	lastUseMap := make(map[*op][]*op) // op of last use -> (canonical) dead ops
	for def, lastUse := range lastUses {
		lastUseMap[lastUse] = append(lastUseMap[lastUse], def)
	}

	// Prepare for assignments
	regUsed := make([]regSet, len(ops)) // In-use register at each op
	for i := range regUsed {
		// X0/Y0/Z0 is reserved by the Go ABI
		regUsed[i].used(locReg{regClassZ, 0})
	}
	locs := make(map[*op]loc)
	assign := func(o *op, l loc) {
		if have, ok := locs[o]; ok {
			fatalf("op %+v already assigned location %v (new %v)", o, have, l)
			return
		}
		if o == canon[o] {
			// Mark this location used over o's live range
			for i := opIndexes[o]; i < opIndexes[lastUses[o]]; i++ {
				regUsed[i].used(l)
			}
		}
		locs[o] = l
	}

	// Assign fixed locations
	id := 0
	for _, o := range ops {
		switch o.op {
		case "arg":
			assign(o, locMem{locReg{cls: regClassFixed, reg: regFP}, o.c.(int), o.name})
		case "argReg":
			assign(o, o.c.(locReg))
		case "const":
			name := o.name
			if name == "" {
				name = fmt.Sprintf("%s_%d<>", nameBase, id)
				id++
			}
			assign(o, locMem{locReg{cls: regClassFixed, reg: regSB}, 0, name})
		case "return":
			assign(o, nil) // no location
			assign(canon[o.args[0]], locReg{regClassZ, 1})
			assign(canon[o.args[1]], locReg{regClassZ, 2})
		}
	}

	// Assign locations.
	for _, o := range ops {
		if traceRegAlloc {
			log.Printf("assign %+v", o)
		}

		if loc, ok := locs[canon[o]]; ok {
			// Already assigned a fixed location above.
			if o != canon[o] {
				assign(o, loc)
			}
			continue
		}

		if canon[o] != o {
			// We should have already visited the canonical op (which is the
			// earliest in the op order) and assigned it a location, so we
			// should never reach here without a location.
			fatalf("canonical op not assigned a location")
		}
		// Below here we know that o is already a canonical op.

		if o.op == "deref" {
			loc, err := locs[o.args[0]].Deref()
			if err != nil {
				log.Fatal(err)
			}
			assign(o, loc)
			continue
		}

		// Assign result location.
		cls := opRegClass[o.op]
		switch cls {
		case '0', '1', '2':
			fatalf("read-modify-write op not canonicalized")
		}
		// Find a free register
		var used uint32
		for i := opIndexes[o]; i < opIndexes[lastUses[o]]; i++ {
			used |= regUsed[i].inUse[cls]
		}

		num := bits.TrailingZeros32(^used)
		switch cls {
		case regClassGP:
			if num >= len(gpRegs) {
				panic("out of GP regs")
			}
		case regClassZ:
			if num >= 32 {
				panic("out of Z regs")
			}
		}
		loc := locReg{cls, num}
		assign(o, loc)
	}

	return locs
}

func emit(w io.Writer, root *op, name string) {
	locs := assignLocs(root, name)

	// Emit constants first
	for o := range root.visit() {
		if o.op == "const" {
			emitConst(w, locs[o].(locMem).name, o.c)
		}
	}

	fmt.Fprintf(w, "TEXT %s(SB), NOSPLIT, $0-0\n", name)

	// Emit body
	for o := range root.visit() {
		switch o.op {
		case "const", "arg", "argReg", "return", "deref":
			// Does not produce code
			continue
		}

		fmt.Fprintf(w, "    %s", o.op)
		if o.op == "VGF2P8AFFINEQB" {
			fmt.Fprintf(w, " $0,")
		}
		for i, arg := range o.args {
			if i == 0 {
				fmt.Fprintf(w, " ")
			} else {
				fmt.Fprintf(w, ", ")
			}
			fmt.Fprintf(w, locs[arg].LocString())
		}
		switch o.op {
		case "VPERMI2B", "VPERMT2B":
			// Read-modify-write instructions, so the output is already in the
			// arguments above.
		default:
			fmt.Fprintf(w, ", %s", locs[o].LocString())
		}
		fmt.Fprintf(w, "\n")
	}
	fmt.Fprintf(w, "    RET\n")
	fmt.Fprintf(w, "\n")
}

func emitFuncTab(w io.Writer, name string, funcs []*Func) {
	fmt.Fprintf(w, "GLOBL %s(SB), RODATA, $%d\n", name, len(funcs)*ptrSize)
	for i, fn := range funcs {
		fmt.Fprintf(w, "DATA  %s+%#02x(SB)/%d, ", name, ptrSize*i, ptrSize)
		if fn == nil {
			fmt.Fprintf(w, "$0\n")
		} else {
			fmt.Fprintf(w, "$%s(SB)\n", fn.name)
		}
	}
	fmt.Fprintf(w, "\n")
}

func emitConst(w io.Writer, name string, data any) {
	rv := reflect.ValueOf(data)
	sz := int(rv.Type().Elem().Size())
	fmt.Fprintf(w, "GLOBL %s(SB), RODATA, $%#x\n", name, rv.Len()*sz)
	for wi := 0; wi < sz*rv.Len()/8; wi++ { // Iterate over words
		var word uint64
		for j := 0; j < 8/sz; j++ { // Iterate over elements in this word
			d := rv.Index(wi*8/sz + j).Uint()
			word |= d << (j * sz * 8)
		}
		fmt.Fprintf(w, "DATA  %s+%#02x(SB)/8, $%#016x\n", name, 8*wi, word)
	}

	// for i := range rv.Len() {
	// 	d := rv.Index(i)
	// 	fmt.Fprintf(w, "DATA  %s+%#02x(SB)/%d, $%#0*x\n", name, i*sz, sz, 2*sz, d)
	// }
	fmt.Fprintf(w, "\n")
}
