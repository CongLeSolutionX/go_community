// Copyright 2024 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

//go:build ignore

package main

import (
	"bytes"
	"cmd/gclab/cnet/gen"
	"fmt"
	"io"
	"log"
	"os"
	"slices"
)

var class_to_size = [...]uint16{0, 8, 16, 24, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 576, 640, 704, 768, 896, 1024, 1152, 1280, 1408, 1536, 1792, 2048, 2304, 2688, 3072, 3200, 3456, 4096, 4864, 5376, 6144, 6528, 6784, 6912, 8192, 9472, 9728, 10240, 10880, 12288, 13568, 14336, 16384, 18432, 19072, 20480, 21760, 24576, 27264, 28672, 32768}
var class_to_allocnpages = [...]uint8{0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 3, 2, 3, 1, 3, 2, 3, 4, 5, 6, 1, 7, 6, 5, 4, 3, 5, 7, 2, 9, 7, 5, 8, 3, 10, 7, 4}

const ptrSize = 8
const ptrBits = 8 * ptrSize
const minSizeForMallocHeader = ptrSize * ptrBits
const pageSize = 8192

const header = "// Code generated by mkasm.go. DO NOT EDIT.\n\n"

func main() {
	generate("condense_amd64.s", genCondensers)
	generate("expand_amd64.s", genExpanders)
}

func generate(fileName string, genFunc func(*gen.File)) {
	var buf bytes.Buffer
	tee := io.MultiWriter(&buf, os.Stdout)

	file := gen.NewFile(tee)

	genFunc(file)

	fmt.Fprintf(tee, header)
	file.Compile()

	f, err := os.Create(fileName)
	if err != nil {
		log.Fatal(err)
	}
	defer f.Close()
	_, err = f.Write(buf.Bytes())
	if err != nil {
		log.Fatal(err)
	}
}

func genCondensers(file *gen.File) {
	gcCondensers := make([]*gen.Func, len(class_to_size))
	for sc, ob := range class_to_size {
		if sc == 0 {
			continue
		}

		totalBytes := int(class_to_allocnpages[sc]) * pageSize
		dartboardBits := totalBytes / 8
		xf := int(ob) / 8
		if xf == 1 {
			continue
		}
		log.Printf("size class %d bytes, condense %dx", ob, xf)

		fn := gen.NewFunc(fmt.Sprintf("condense%d<>", xf))
		ptrDartboard := gen.Arg[gen.Ptr[gen.Uint64x8]](fn)

		condenseAVX512(dartboardBits, xf, ptrDartboard)

		file.AddFunc(fn)
		gcCondensers[sc] = fn
	}

	// Generate table mapping size class to condenser PC
	file.AddConst("·gcCondensers", gcCondensers)
}

func genExpanders(file *gen.File) {
	gcExpanders := make([]*gen.Func, len(class_to_size))
	for sc, ob := range class_to_size {
		if class_to_allocnpages[sc] != 1 {
			// These functions all produce a bitmap that covers exactly one
			// page.
			continue
		}
		if ob > minSizeForMallocHeader {
			// This size class is too big to have a packed pointer/scalar bitmap.
			break
		}

		xf := int(ob) / 8
		log.Printf("size class %d bytes, expansion %dx", ob, xf)

		fn := gen.NewFunc(fmt.Sprintf("expand%d<>", xf))
		ptrObjBits := gen.Arg[gen.Ptr[gen.Uint8x64]](fn)

		if xf == 1 {
			expandIdentity(ptrObjBits)
		} else if !gfExpander(xf, ptrObjBits) {
			// TODO: Temporary while gfExpander sometimes fails
			continue
		}
		file.AddFunc(fn)
		gcExpanders[sc] = fn
	}

	// Generate table mapping size class to expander PC
	file.AddConst("·gcExpanders", gcExpanders)
}

func fatalf(f string, args ...any) {
	panic(fmt.Sprintf(f, args...))
}

// mat8x8 is an 8x8 bit matrix.
type mat8x8 struct {
	mat [8]uint8
}

func matGroupToVec(mats *[8]mat8x8) [8]uint64 {
	var out [8]uint64
	for i, mat := range mats {
		for j, row := range mat.mat {
			// For some reason, Intel flips the rows.
			out[i] |= uint64(row) << ((7 - j) * 8)
		}
	}
	return out
}

// condenser functions take an n-bit bitmap that consists of a sequence of f-bit
// fields and populate an output (n/f)-bit bitmap where bit i is set if any of
// the bits of field i are set.
//
// The input is
//
//  AX *[...]uint64 = A pointer to the n bit input
//
// The output is
//
//  Z1 [64]uint8    = The (n/f)-bit output (up to 512 bits)
//
// In practice, the input is a per-word dartboard bitmap for a span and the
// output is a per-object bitmap. Since this is driven by size classes, we can
// make some assumptions about the parameter space.
//
// - 1024 <= n <= 10240. The smallest spans are 1 page = 8192 bytes, or 1024
// bits of bitmap. The largest spans are 10 pages = 81920 bytes, or 10240 bits.
// In terms of uint64s, 16 <= n/64 <= 160. And in terms of AVX-512 registers, 2
// <= n/512 <= 20.
//
// - 1 <= f <= 4096. This is mainly to say that f can be quite large.
//
// - n/f <= 1024. That is, the output is at most 1024 bits. However, at f == 1,
// this is an identity function, so we take a different path entirely.
//
// - At bytes >= 16: f >= 2 and n/f <= 512. That is, the output will fit in a
// single AVX-512 register.
//
// - At bytes >= 128: f >= 16 and n/f <= 64. That is, the output will fit in a
// single uint64.
//
// - At bytes >= 768: f is always a multiple of 16 and n/f <= 11. Often, f is a
// multiple of a higher power of 2.
//
// - If f >= 4, f is a multiple of 2. If f >= 32, f is a multiple of 4.
//   If f >= 64, f is a multiple of 8. If f >= 96, f is a multiple of 16.

// TODO: How should we deal with partial spans at higher size classes? 1) In my
// pure Go implementation, I generate condensers for each prefix and each
// suffix. I could do that here. 2) I could generate a condenser for each
// individual page of a span, which would be half as many, though they'd have to
// take the condensed bitmap as an input and OR their results in. Maybe that's
// not so bad, and at that point f is a multiple of 16. 3) I could just pass in
// the span's whole bitmap and allow the racy reads of the part outside the
// region, and then mask off the result.

func condenseAVX512(n, f int, ptrDartboard gen.Ptr[gen.Uint64x8]) {
	// Create a list of all fields. Fields are also split at 64 bit boundaries
	// at this point.
	type field struct {
		startBit int // field covers bits [startBit, endBit]
		endBit   int

		outBit int // Bit to set in the output
	}
	fields := make([][]field, (n+512-1)/512)
	for outBit := 0; outBit < n/f; outBit++ {
		startBit := outBit * f
		endBit := startBit + (f - 1)

		// The field may be split across two or more 64 bit fields. Emit
		// everything but the trailing piece.
		for startBit/64 != endBit/64 {
			endBit1 := startBit | (64 - 1)
			fields[startBit/512] = append(fields[startBit/512], field{startBit, endBit1, outBit})
			startBit = endBit1 + 1
		}
		// Emit the trailing piece. If the field isn't split, this will be the
		// only piece.
		fields[startBit/512] = append(fields[startBit/512], field{startBit, endBit, outBit})

		// Note: It's possible for a split field to be just a single bit. This
		// is weird but ultimately harmless.
	}

	// Process 512 bits of input at a time.
	var objBits gen.Uint64x8
	var objBitsMask gen.Mask8
	for startBit := 0; startBit < n; startBit += 512 {
		i := startBit / 512
		fields1 := fields[i]
		startByte := startBit / 8

		// Load this 512 bit chunk of the dartboard.
		dartboard := gen.Deref(ptrDartboard.AddConst(startByte))

		// TODO: For f == 2, it's better to just shift and or.
		//
		// TODO: For power-of-2 f, 8 <= f <= 64, use CMP to reduce.
		//
		// TODO: For f >= 8, does it make sense to VPCMPB on the bytes that
		// don't contain a field boundary? We could VPERMB to pull out the whole
		// bytes and put them in the right elements. If we also VPERMB the bytes
		// containing boundaries, could we take advantage of the space we freed
		// up to ensure no field spans a 64 bit boundary?

		// Create two consts:
		//
		// highBits has the high bit of every field set. This high bit acts as a
		// "borrow boundary" for the subtraction we're about to do.
		//
		// ones has a "1" in each field.
		var highBits, ones [8]uint64
		for _, field := range fields1 {
			highBits[(field.endBit-startBit)/64] |= 1 << (field.endBit % 64)
			ones[(field.startBit-startBit)/64] |= 1 << (field.startBit % 64)
		}

		// Set the high bit in each field.
		highBitConst := gen.ConstUint64x8(highBits, fmt.Sprintf("*_high%d", i))
		d1 := dartboard.Or(highBitConst)

		// Subtract 1 from each field. This will clear the high bit only if all
		// bits in the field are 0. In other words, the high bit of the field
		// will be the OR of all of the lower bits in the field.
		onesConst := gen.ConstUint64x8(ones, fmt.Sprintf("*_ones%d", i))
		d2 := d1.Sub(onesConst)

		// OR back in the original bitmap. This OR's each high bit with the
		// original high bit, which we lost when we set all the high bits to 1.
		d3 := d2.Or(dartboard)

		// Finally, gather up the high bits of each field.
		//
		// For f < 64, fields will be split over at most 2 64-bit elements. Just
		// do two bit extractions, placing the extracted bits in their final
		// places, and OR the results together. For f == 64, we can use a
		// VPCMPQ. For f > 64, f is always a multiple of 8.
		//
		// TODO: The code this generates for f==3 is hair-raising. It might
		// actually be better to do some bit shifts (VPSHRDVW) to avoid
		// splitting fields across 64-bit boundaries.
		if f < 64 {
			// Fields will be split over at most 2 64-bit elements. We just do
			// up to two bit extractions, placing the extracted bits in their
			// final places, and OR the results together.
			startOutBit := fields1[0].outBit
			var bits0, bits1 []int
			tryAdd := func(bits *[]int, f field) bool {
				if len(*bits) > f.outBit-startOutBit {
					return false
				}
				for len(*bits) < (f.outBit - startOutBit) {
					*bits = append(*bits, -1) // Don't care
				}
				*bits = append(*bits, f.endBit-startBit)
				return true
			}
			for _, field := range fields1 {
				if !(tryAdd(&bits0, field) || tryAdd(&bits1, field)) {
					fatalf("more than two fields map to bit %d", field.outBit)
				}
			}
			// Gather 64 bits at a time.
			for off := 0; off < len(bits0); off += 64 {
				outBit := startOutBit + off
				k1 := extractBits(d3, bits0[off:], fmt.Sprintf("*_field%dLo", outBit))
				if len(bits1) > off {
					k2 := extractBits(d3, bits1[off:], fmt.Sprintf("*_field%dHi", outBit))
					k1 = k1.Or(k2)
				}

				// Write k1 to the output
				if (objBits == gen.Uint64x8{}) {
					if outBit != 0 {
						fatalf("first output starts at bit %d, expected 0", outBit)
					}
					objBitsMask = gen.ConstMask8(1)
					objBits = gen.BroadcastUint64x8Zeroed(k1.ToUint64(), objBitsMask)
				} else {
					if outBit%64 == 0 {
						// Easy case. We're inserting on a 64 bit boundary, so
						// we can use a "broadcast" masked to a single 64 bit
						// element.
						m := objBitsMask.ShiftLeft(uint8(outBit / 64))
						objBits = objBits.BroadcastMasked(k1.ToUint64(), m)
					} else {
						// Construct the low bits
						m := objBitsMask.ShiftLeft(uint8(outBit / 64))
						loBits := gen.BroadcastUint64x8Zeroed(k1.ShiftLeft(uint8(outBit%64)).ToUint64(), m)
						// Combine in the high bits
						m = objBitsMask.ShiftLeft(1 + uint8(outBit/64))
						allBits := loBits.BroadcastMasked(k1.ShiftRight(uint8(64-outBit%64)).ToUint64(), m)
						// Combine this into the overall output
						objBits = objBits.Or(allBits)
					}
				}
			}
		}

	}
	if (objBits != gen.Uint64x8{}) {
		gen.Return(objBits)
	}

	// TODO
}

func extractBits(src gen.Uint64x8, bits []int, name string) gen.Mask64 {
	log.Println(bits)

	// In true AVX building-with-Tetris-blocks fashion, we can gather up to 64
	// bits at a time, *but* each group of 8 bits in the result must come from
	// the corresponding group of 64 bits in the input.
	//
	// Specifically, each element in a [8]uint8 lane selects 8 bits from the
	// corresponding uint64 lane.
	//
	// Hence, we most likely need to do a byte shuffle first to get each bit
	// into the right 64-bit lane.
	var byteShuf [64]uint8
	var bitShuf [64]uint8
	var mask uint64 = ^uint64(0)
	allDontCare := true
	for outBit := range byteShuf {
		if outBit >= len(bits) || bits[outBit] == -1 {
			// The shuffle values don't matter.
			byteShuf[outBit] = 0xff
			bitShuf[outBit] = 0xff
			// Mask it out of the final result.
			mask &^= 1 << outBit
			continue
		}
		allDontCare = false
		// Shuffle the byte containing this bit (srcBit/8) into the outBit'th
		// byte.
		srcBit := bits[outBit]
		if srcBit < 0 || srcBit >= 512 {
			fatalf("source bit %d out of range [0, 512)", srcBit)
		}
		// TODO: This can needlessly duplicate bytes if there are multiple bits
		// in the same byte. It's harmless, but if I avoid that the code and
		// masks may be more readable.
		byteShuf[outBit] = uint8(srcBit / 8)
		// Calculate the post-shuffle bit index.
		bitOff := outBit*8 + int(srcBit%8)
		if outBit/8 != bitOff/64 {
			fatalf("generated cross-lane bit shuffle: original %d -> %d, post-shuffle %d -> %d", srcBit, outBit, bitOff, outBit)
		}
		bitShuf[outBit] = uint8(bitOff) % 64
	}
	if allDontCare {
		fatalf("all bits are don't-care")
	}

	byteShuffled := src.ToUint8x64().Shuffle(gen.ConstUint8x64(byteShuf, name+"ByteShuf")).ToUint64x8()
	if mask == ^uint64(0) {
		return byteShuffled.ShuffleBits(gen.ConstUint8x64(bitShuf, name+"BitShuf"))
	}
	return byteShuffled.ShuffleBitsMasked(gen.ConstUint8x64(bitShuf, name+"BitShuf"), gen.ConstMask64(mask))
}

// expandIdentity implements 1x expansion (that is, no expansion).
func expandIdentity(ptrObjBits gen.Ptr[gen.Uint8x64]) {
	objBitsLo := gen.Deref(ptrObjBits)
	objBitsHi := gen.Deref(ptrObjBits.AddConst(64))
	gen.Return(objBitsLo, objBitsHi)
}

// gfExpander produces a function that expands each bit in an input bitmap into
// f consecutive bits in an output bitmap.
//
// The input is
//
//	AX *[8]uint64 = A pointer to floor(1024/f) bits (f >= 2, so at most 512 bits)
//
// The output is
//
//	Z1 [64]uint8  = The bottom 512 bits of the expanded bitmap
//	Z2 [64]uint8  = The top 512 bits of the expanded bitmap
//
// TODO: This should Z0/Z1.
func gfExpander(f int, ptrObjBits gen.Ptr[gen.Uint8x64]) bool {
	// TODO: For powers of 2 >= 8, we can use mask expansion ops to make this much simpler.

	// TODO: For f >= 8, I suspect there are better ways to do this.
	//
	// For example, we could use a mask expansion to get a full byte for each
	// input bit, and separately create the bytes that blend adjacent bits, then
	// shuffle those bytes together. Certainly for f >= 16 this makes sense
	// because each of those bytes will be used, possibly more than once.

	if f >= 12 {
		// TODO: Code gen fails right now for most of these because the shuffle
		// has too many inputs. Sit down and think about whether this approach
		// is actually right at all for these larger factors.
		return false
	}

	objBits := gen.Deref(ptrObjBits)

	type term struct {
		iByte, oByte int
		mat          mat8x8
	}
	var terms []term

	// Iterate over all output bytes and construct the 8x8 GF2 matrix to compute
	// the output byte from the appropriate input byte. Gather all of these into
	// "terms".
	for oByte := 0; oByte < 1024/8; oByte++ {
		var byteMat mat8x8
		iByte := -1
		for oBit := oByte * 8; oBit < oByte*8+8; oBit++ {
			iBit := oBit / f
			if iByte == -1 {
				iByte = iBit / 8
			} else if iByte != iBit/8 {
				fatalf("output byte %d straddles input bytes %d and %d", oByte, iByte, iBit/8)
			}
			// One way to view this is that the i'th row of the matrix will be
			// ANDed with the input byte, and the parity of the result will set
			// the i'th bit in the output. We use a simple 1 bit mask, so the
			// parity is irrelevant beyond selecting out that one bit.
			byteMat.mat[oBit%8] = 1 << (iBit % 8)
		}
		terms = append(terms, term{iByte, oByte, byteMat})
	}

	if false {
		// Print input byte -> output byte as a matrix
		maxIByte, maxOByte := 0, 0
		for _, term := range terms {
			maxIByte = max(maxIByte, term.iByte)
			maxOByte = max(maxOByte, term.oByte)
		}
		iToO := make([][]rune, maxIByte+1)
		for i := range iToO {
			iToO[i] = make([]rune, maxOByte+1)
		}
		matMap := make(map[mat8x8]int)
		for _, term := range terms {
			i, ok := matMap[term.mat]
			if !ok {
				i = len(matMap)
				matMap[term.mat] = i
			}
			iToO[term.iByte][term.oByte] = 'A' + rune(i)
		}
		for o := range maxOByte + 1 {
			fmt.Printf("%d", o)
			for i := range maxIByte + 1 {
				fmt.Printf(",")
				if mat := iToO[i][o]; mat != 0 {
					fmt.Printf("%c", mat)
				}
			}
			fmt.Println()
		}
	}

	// In hardware, each (8 byte) matrix applies to 8 bytes of data in parallel,
	// and we get to operate on up to 8 matrixes in parallel (or 64 values). That is:
	//
	//  abcdefgh ijklmnop qrstuvwx yzABCDEF GHIJKLMN OPQRSTUV WXYZ0123 456789_+
	//    mat0     mat1     mat2     mat3     mat4     mat5     mat6     mat7

	// Group the terms by matrix, but limit each group to 8 terms.
	const termsPerGroup = 8       // Number of terms we can multiply by the same matrix.
	const groupsPerSuperGroup = 8 // Number of matrixes we can fit in a vector.

	matMap := make(map[mat8x8]int)
	allMats := make(map[mat8x8]bool)
	var termGroups [][]term
	for _, term := range terms {
		allMats[term.mat] = true

		i, ok := matMap[term.mat]
		if ok && f > groupsPerSuperGroup {
			// The output is ultimately produced in two [64]uint8 registers.
			// Getting every byte in the right place of each of these requires a
			// final permutation that often requires more than one source.
			//
			// Up to 8x expansion, we can get really nice grouping so we can use
			// the same 8 matrix vector several times, without producing
			// permutations that require more than two sources.
			//
			// Above 8x, however, we can't get nice matrixes anyway, so we
			// instead prefer reducing the complexity of the permutations we
			// need to produce the final outputs. To do this, avoid grouping
			// together terms that are split across the two registers.
			outRegister := termGroups[i][0].oByte / 64
			if term.oByte/64 != outRegister {
				ok = false
			}
		}
		if !ok {
			// Start a new term group.
			i = len(termGroups)
			matMap[term.mat] = i
			termGroups = append(termGroups, nil)
		}

		termGroups[i] = append(termGroups[i], term)

		if len(termGroups[i]) == termsPerGroup {
			// This term group is full.
			delete(matMap, term.mat)
		}
	}

	for i, termGroup := range termGroups {
		log.Printf("term group %d:", i)
		for _, term := range termGroup {
			log.Printf("  %+v", term)
		}
	}

	// We can do 8 matrix multiplies in parallel, which is 8 term groups. Pack
	// as many term groups as we can into each super-group to minimize the
	// number of matrix multiplies.
	//
	// Ideally, we use the same matrix in each super-group, which might mean
	// doing fewer than 8 multiplies at a time. That's fine because it never
	// increases the total number of matrix multiplies.
	//
	// TODO: Packing the matrixes less densely may let us use more broadcast
	// loads instead of general permutations, though. That replaces a load of
	// the permutation with a load of the matrix, but is probably still slightly
	// better.
	var sgSize, nSuperGroups int
	oneMatVec := f <= groupsPerSuperGroup
	if oneMatVec {
		// We can use the same matrix in each multiply by doing sgSize
		// multiplies at a time.
		sgSize = groupsPerSuperGroup / len(allMats) * len(allMats)
		nSuperGroups = (len(termGroups) + sgSize - 1) / sgSize
	} else {
		// We can't use the same matrix for each multiply. Just do as many at a
		// time as we can.
		//
		// TODO: This is going to produce several distinct matrixes, when we
		// probably only need two. Be smarter about how we create super-groups
		// in this case. Maybe we build up an array of super-groups and then the
		// loop below just turns them into ops?
		sgSize = 8
		nSuperGroups = (len(termGroups) + groupsPerSuperGroup - 1) / groupsPerSuperGroup
	}

	// Construct each super-group.
	var matGroup [8]mat8x8
	var matMuls []gen.Uint8x64
	var perm [128]int
	for sgi := range nSuperGroups {
		var iperm [64]uint8
		for i := range iperm {
			iperm[i] = 0xff // "Don't care"
		}
		// Pick off sgSize term groups.
		superGroup := termGroups[:min(len(termGroups), sgSize)]
		termGroups = termGroups[len(superGroup):]
		// Build the matrix and permutations for this super-group.
		var thisMatGroup [8]mat8x8
		for i, termGroup := range superGroup {
			// All terms in this group have the same matrix. Pick one.
			thisMatGroup[i] = termGroup[0].mat
			for j, term := range termGroup {
				// Build the input permutation.
				iperm[i*termsPerGroup+j] = uint8(term.iByte)
				// Build the output permutation.
				perm[term.oByte] = sgi*groupsPerSuperGroup*termsPerGroup + i*termsPerGroup + j
			}
		}
		log.Printf("input permutation %d: %v", sgi, iperm)

		// Check that we're not making more distinct matrixes than expected.
		if oneMatVec {
			if sgi == 0 {
				matGroup = thisMatGroup
			} else if matGroup != thisMatGroup {
				fatalf("super-groups have different matrixes:\n%+v\n%+v", matGroup, thisMatGroup)
			}
		}

		// Emit matrix op.
		matConst := gen.ConstUint64x8(matGroupToVec(&thisMatGroup), fmt.Sprintf("*_mat%d<>", sgi))
		inOp := objBits.Shuffle(gen.ConstUint8x64(iperm, fmt.Sprintf("*_inShuf%d<>", sgi)))
		matMul := matConst.GF2P8Affine(inOp)
		matMuls = append(matMuls, matMul)
	}

	log.Printf("output permutation: %v", perm)

	outLo := genShuffle("*_outShufLo", (*[64]int)(perm[:64]), matMuls...)
	outHi := genShuffle("*_outShufHi", (*[64]int)(perm[64:]), matMuls...)
	gen.Return(outLo, outHi)

	return true
}

func genShuffle(name string, perm *[64]int, args ...gen.Uint8x64) gen.Uint8x64 {
	// Construct flattened permutation.
	var vperm [64]byte
	// Get the inputs used by this permutation.
	var inputs []int
	for i, src := range perm {
		inputIdx := slices.Index(inputs, src/64)
		if inputIdx == -1 {
			inputIdx = len(inputs)
			inputs = append(inputs, src/64)
		}
		vperm[i] = byte(src%64 | (inputIdx << 6))
	}
	// Emit instruction.
	constOp := gen.ConstUint8x64(vperm, name)
	if len(inputs) == 1 {
		return args[inputs[0]].Shuffle(constOp)
	} else if len(inputs) == 2 {
		return args[inputs[0]].Shuffle2(args[inputs[1]], constOp)
	}
	panic("> 2 inputs not implemented")
}
