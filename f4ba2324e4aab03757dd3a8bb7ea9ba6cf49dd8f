{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "213c8465_ef9bc0d0",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5976
      },
      "writtenOn": "2022-12-05T19:59:24Z",
      "side": 1,
      "message": "TryBots beginning. Status page: https://farmer.golang.org/try?commit\u003df4ba2324\n",
      "tag": "autogenerated:trybots~beginning",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "3f22bdf9_78c643a5",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 5976
      },
      "writtenOn": "2022-12-05T20:14:36Z",
      "side": 1,
      "message": "TryBots are happy.\n\n",
      "parentUuid": "213c8465_ef9bc0d0",
      "tag": "autogenerated:trybots~happy",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "4dadbf11_df0a2837",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 30738
      },
      "writtenOn": "2022-12-05T20:40:12Z",
      "side": 1,
      "message": "regarding the e.Cap()/4 comparison- does the compiler transform this into an *4 comparison the other way round or are divisions faster these days?",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "54d7fdd4_68864f57",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 8495
      },
      "writtenOn": "2022-12-05T20:42:02Z",
      "side": 1,
      "message": "That optimization is in place:\n```\nsink \u003d src.Cap() / 4\n```\ncompiles to:\n```\n0x0000 00000 (/tmp/sandbox2160700864/main.go:12)\tMOVQ\tmain.src+16(SB), AX\n0x0007 00007 (/tmp/sandbox2160700864/main.go:12)\tSHRQ\t$2, AX\n0x000b 00011 (/tmp/sandbox2160700864/main.go:12)\tMOVQ\tAX, main.sink(SB)\n```",
      "parentUuid": "4dadbf11_df0a2837",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "7b4f41ae_e010d45d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 8495
      },
      "writtenOn": "2022-12-05T20:44:27Z",
      "side": 1,
      "message": "To be specific, it doesn\u0027t translate it into a *4 otherwise that can overflow. Instead it performs a right bit-shift by 2.",
      "parentUuid": "54d7fdd4_68864f57",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "54a29f4e_d603e2a3",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 30738
      },
      "writtenOn": "2022-12-05T20:48:08Z",
      "side": 1,
      "message": "With this CL, the size of the buffers in the pool will likely be in the „utilized“ size. That size (maybe the size of the last buffer put back into the pool) might be used to pre-allocate the buffer‘s buf memory before putting an un-utilised recycled buffer back into the pool to save on future allocations for buffer growth?",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "7a62ceb3_72ab9f1a",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 8495
      },
      "writtenOn": "2022-12-05T20:54:00Z",
      "side": 1,
      "message": "That can be future optimization. I left it out because the addition of another field to track the previous length increases the size of `encodeState` past 128B, which bumps it up to the next allocation size class (see https://commaok.xyz/post/discovering-size-classes/). This has other effects on memory cache locality and what not.\n\nThere\u0027s other work that can be done to optimize this further. For example, the scratch buffer is redundant with the fact that `bytes.Buffer` itself has a scratch buffer. Removing that will reduce the size of `encodeState`, which allows us to track other statistics.",
      "parentUuid": "54a29f4e_d603e2a3",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "8d4a6cbe_bea703f5",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 34561
      },
      "writtenOn": "2022-12-06T19:56:24Z",
      "side": 1,
      "message": "Sorry for so many comments. Thought of one more idea.\n\nThis is kind of a big deal for my codebase (kube-apiserver); if you think the go team would have appetite for it, I would be willing to write the segmented buffer solution for this. Are you interested in that?\n\nPS My very rough estimate is that kube-apiserver gets maybe 20% utilization right now.",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "4191020c_6c65c6ef",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 34561
      },
      "writtenOn": "2022-12-07T07:02:21Z",
      "side": 1,
      "message": "I tried implementing the segmented buffer approach here: https://go-review.git.corp.google.com/c/go/+/455776",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "65cbe699_0492de76",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 8495
      },
      "writtenOn": "2022-12-07T19:06:05Z",
      "side": 1,
      "message": "I apologize that my response rate will be declining. I\u0027ll be going on paternity leave soon and there\u0027s non-contribution work of mine that takes priority.",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b00f27fe_f37ef109",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 165,
      "author": {
        "id": 34561
      },
      "writtenOn": "2022-12-06T19:56:24Z",
      "side": 1,
      "message": "Idea: if the buffer is sufficiently large, instead of copying this, return e.Bytes() and reset the buffer. If `e` tracks average size then this decision is easier to make. If not, maybe do this for any buffer larger than 1MiB?",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "58e84a0d_87d9a472",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 325,
      "author": {
        "id": 34561
      },
      "writtenOn": "2022-12-05T21:37:02Z",
      "side": 1,
      "message": "However... If you solve for max memory held by the pool, it\u0027s still unbounded, no? And I think that is more important than utilization when we\u0027re worried about memory problems. (put another way, everything in the pool actually has zero utilization while it\u0027s in the pool)\n\nThis is a significant improvement, but have you considered instead something like:\n* keep a running average and std dev (of the buffer len) in global memory\n* drop buffers with caps larger than e.g average + 3 * stddev\n\nI can suggest various ways of adjusting that cap if you want more ideas along these lines.",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "0e5fc633_74644ac2",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 325,
      "author": {
        "id": 8495
      },
      "writtenOn": "2022-12-05T21:49:22Z",
      "side": 1,
      "message": "\u003e (put another way, everything in the pool actually has zero utilization while it\u0027s in the pool)\n\nI\u0027m not sure that\u0027s the right way of looking at this since that would imply we should never use a sync.Pool.\n\nUnused items in the sync.Pool eventually get freed by the GC. It doesn\u0027t get freed immediately in light of https://go.dev/issue/22950, but does so within two GCs.\n\n\u003e keep a running average and std dev (of the buffer len) in global memory\n\u003e drop buffers with caps larger than e.g average + 3 * stddev\n\nUse of global state has been considered before (the discussion has long been spread across multiple CLs and different GH issues). We should avoid any solution that relies on global memory as that leads to contention in highly concurrent applications. The statistics gathering has to be cheap and fast enough that it isn\u0027t a notable fraction of CPU time when marshaling tiny objects. Computing a running average and the standard deviation are not particularly cheap operations.",
      "parentUuid": "58e84a0d_87d9a472",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6426e9bf_755e382d",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 325,
      "author": {
        "id": 34561
      },
      "writtenOn": "2022-12-05T22:12:29Z",
      "side": 1,
      "message": "I like this change better with that context, thanks.\n\nIt still seems like a distribution of requests like this causes a lot of extra memory usage:\n* 10% of requests: 1MiB\n* 16% of requests: 257MiB (these reset the strikes but don\u0027t take advantage of all the memory)\n* 74% of requests: 1kiB\n\nIt seems like the fundamental problem is that it\u0027s unknowable how much space a given work item will need. The solution that comes to mind to actually fix this looks like:\n* keep multiple pools, e.g. 1 per base-10 order-of-magnitude, starting at e.g. 1kiB\n* When buffer needs to reallocate, it first tries to get a buffer from the next size up and if successful, copies instead, releasing the original buffer back into the appropriate pool.\n\nI\u0027m not sure if that can be changed without modifying Buffer, unfortunately",
      "parentUuid": "0e5fc633_74644ac2",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a6d9b497_abf43415",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 325,
      "author": {
        "id": 8495
      },
      "writtenOn": "2022-12-05T22:38:25Z",
      "side": 1,
      "message": "The main advantage of providing a sufficiently large buffer up front is that we avoid the copying as we jump buffer sizes (e.g., 1K to 2K to 4K to 8K, etc). Admittedly the amortized cost of this is O(2n) where n is the final output size. (Technically, it\u0027s O(3n) since there\u0027s a final clone at the end to avoid leaking the pooled buffer outside the json package).\n\nA variation on the idea you suggested is to have a custom `Buffer` implementation that uses a series of segmented buffers under the hood. Every time we exhaust the current buffer, we grab a larger buffer from the higher-level pool. However, we do *NOT* copy the contents from the smaller buffer to the larger buffer, but treat the final output as the concatenation of all the buffers. When `Marshal` returns, we construct the final buffer from the segments. Counting the final clone, this approach would be O(2n) instead of O(3n).\n\nThat said, I think the segmented buffer with multiple pools can be something that\u0027s explored in the future. It\u0027ll definitely be a more complex change that what\u0027s presented here.",
      "parentUuid": "6426e9bf_755e382d",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "7e1fa0b3_5fc73c7d",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 325,
      "author": {
        "id": 34561
      },
      "writtenOn": "2022-12-05T22:55:12Z",
      "side": 1,
      "message": "(I agree with your analysis-- I did suggest 10x instead of 2x which might help)\n\nI agree a segmented buffer would be great, but given go\u0027s lack of operator overloading it would be a very leaky abstraction for consumers.\n\nI guess my hesitation with the current approach is that it\u0027s not actually clear what the utilization is right now and therefore it\u0027s hard to evaluate how much of an improvement this would be. E.g. I think it\u0027s still possible for the pool to pin arbitrary amounts of memory given the right request distribution and frequency.",
      "parentUuid": "a6d9b497_abf43415",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "91fb9907_fa5cfdee",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 325,
      "author": {
        "id": 8495
      },
      "writtenOn": "2022-12-05T23:36:35Z",
      "side": 1,
      "message": "\u003e still possible for the pool to pin arbitrary amounts of memory given the right request distribution and frequency.\n\nTrue, the part that\u0027s unbounded though is dependent on caller behavior. Even if `sync.Pool` were not involved, the minimum heap usage will be N number of concurrent `json.Marshal` calls multiplied by M average bytes of output per call. Since N is outside the control of `json.Marshal`, that could be arbitrarily large and unbounded. This CL makes it such that the worst-case heap usage of the json package is some constant fraction of N*M (when it previously was unbounded even if N and M were bounded). At least N is within user control.\n\nWe could add a weighted semaphore in the `json` package that slows the allocation rate to be within some maximum amount. However, it\u0027s not clear to me that this belongs in the json package. Arguably every Go package that allocates on behalf of the caller could then be justified to include a weighted semaphore. That does not seem to scale. It seems like this should either be a system-wide runtime feature OR a weighted semaphore that exists on the caller side.\n\nI should note that a limiter to the system memory at the runtime level now exists in with the introduction of GOMEM in https://github.com/golang/go/issues/48409, which enforces a soft memory limit for a Go process. When the total heap exceeds the memory limit, the GC is run more often to avoid OOMs. Consequently, this causes unused objects in `sync.Pools` to be flushed more often.\n\nAt Tailscale, we had a service that was OOMing often even though we had weighted semaphores to reduce the allocation rate. Fundamentally, such a mechanism is flawed since the weighted semaphore doesn\u0027t know that a large buffer has been truly freed by the Go runtime (i.e., there\u0027s an asynchronous nature to when objects are actually freed and versus when a local piece of code stops using an object). Assistance from the Go runtime is necessary. The effects of setting GOMEM was immediately helpful since it caused the GC to run more frequently during spikes.",
      "parentUuid": "7e1fa0b3_5fc73c7d",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "35ef8639_740f93fd",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 325,
      "author": {
        "id": 34561
      },
      "writtenOn": "2022-12-06T01:10:18Z",
      "side": 1,
      "message": "\u003e\u003eI guess my hesitation with the current approach is that it\u0027s not actually clear what the utilization is right now and therefore it\u0027s hard to evaluate how much of an improvement this would be.\n\nI wasn\u0027t clear, sorry. Let me try again: if at least 1/4th of requests are for big buffers, then this code change will not result in a behavior change.\n\nCan we measure something to see if, in practice, this is a good threshold to bake into the code?",
      "parentUuid": "91fb9907_fa5cfdee",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "aaa3769f_e978f998",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 325,
      "author": {
        "id": 8495
      },
      "writtenOn": "2022-12-07T19:06:05Z",
      "side": 1,
      "message": "The thresholds could certainly use fine-tuning and it\u0027s a bit challenging to come up with constants that are ideal for all use-cases.",
      "parentUuid": "35ef8639_740f93fd",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "32b466ee_1411d40e",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 330,
      "author": {
        "id": 34561
      },
      "writtenOn": "2022-12-06T01:16:57Z",
      "side": 1,
      "message": "A much shorter way to state my complaint: I doubt real-life systems currently waste 95% of their encoding-related memory and therefore I doubt this bound is a real-life improvement; how can we measure to find out?",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "73cf4588_1f571a45",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 336,
      "author": {
        "id": 34561
      },
      "writtenOn": "2022-12-06T01:10:18Z",
      "side": 1,
      "message": "Alternative idea for computing utilization:\n\n// track how much memory the last 8 uses of this buffer actually used on average.\nif e.lenAverage \u003d\u003d 0 {\n  e.lenAverage \u003d e.Len()\n} else { \n  e.lenAverage \u003d e.lenAverage + (e.Len() / 8) - (e.lenAverage / 8)\n}\n\n// if Cap is over 4x the average size, don\u0027t keep it.\nif e.Cap() \u003e e.lenAverage * 4 { /* discard buffer */ }\n\n(adjust constants to taste / measurement)\n\nThis can be combined with the strikes system -- I think tracking slightly more usage data means it\u0027s safe to give only 1 or 2 strikes instead of 4.",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "6ef4451a_71af587a",
        "filename": "src/encoding/json/encode.go",
        "patchSetId": 1
      },
      "lineNbr": 337,
      "author": {
        "id": 34561
      },
      "writtenOn": "2022-12-06T01:10:18Z",
      "side": 1,
      "message": "Alternative idea: don\u0027t reset this.\n\nAlternative idea: have a counter for too small and a counter for not too small, and reset the buffer to zero when those counters differ by 50%.",
      "revId": "f4ba2324e4aab03757dd3a8bb7ea9ba6cf49dd8f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    }
  ]
}