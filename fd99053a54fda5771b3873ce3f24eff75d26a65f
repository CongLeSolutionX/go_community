{
  "comments": [
    {
      "key": {
        "uuid": "039d10ef_32134a32",
        "filename": "/COMMIT_MSG",
        "patchSetId": 1
      },
      "lineNbr": 7,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-11T19:13:17Z",
      "side": 1,
      "message": "This all equally applies to syscalls too, right?",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "932a0fd9_b149aa82",
        "filename": "/COMMIT_MSG",
        "patchSetId": 1
      },
      "lineNbr": 7,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T04:13:51Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "039d10ef_32134a32",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "0973f49b_1ec935f0",
        "filename": "misc/cgo/test/issue28701.go",
        "patchSetId": 1
      },
      "lineNbr": 23,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-11T19:31:08Z",
      "side": 1,
      "message": "A test with 1 second timeout looks like recipe for flakiness. It will sure happen that a test VM will just not schedule this whole thing for a second once in a while.\nIn few runs of the previous test, we got few timeouts with much larger duration, right? What was that? 1 minute?\nI would increase this timeout right away.",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "a5116300_a806986f",
        "filename": "misc/cgo/test/issue28701.go",
        "patchSetId": 1
      },
      "lineNbr": 23,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T04:13:51Z",
      "side": 1,
      "message": "Ack. I tweaked the constants a bit here. This test now reliably takes 30s+ without the scheduler patch, and about 130ms with the scheduler patch. I upped the timeout to 10s. Do you think that\u0027ll be sufficient to avoid flakes?",
      "parentUuid": "0973f49b_1ec935f0",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "95f83bde_10b6ff9e",
        "filename": "misc/cgo/test/issue28701.go",
        "patchSetId": 1
      },
      "lineNbr": 23,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-12T19:33:53Z",
      "side": 1,
      "message": "I don\u0027t know. We will see. Hopefully.",
      "parentUuid": "a5116300_a806986f",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "2c4bb4da_499b2f9a",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 1380,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-11T19:31:08Z",
      "side": 1,
      "message": "Also reset p.oldm here?\nIt would be nice to have a sanity check somewhere that we don\u0027t left p.oldm. Maybe pidleput or reentersyscall before we set it? Please test that the check will catch this place, and stopTheWorldWithSema if you comment p.oldm\u003d0 out.",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "e9454590_4e9bd913",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 1380,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T04:13:51Z",
      "side": 1,
      "message": "Good catch, thanks. I added the check to pidleput. reentersyscall is nosplit so attempting to print there results in an unhelpful morestack explosion.",
      "parentUuid": "2c4bb4da_499b2f9a",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "18f51c38_45a1cd3c",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 2833,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-11T19:50:32Z",
      "side": 1,
      "message": "Add a comment here stating that this needs to be reset to 0 by whoever CASes p.status from _Psyscall.\nThat\u0027s the idea, right?",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "01159e64_8a59cba0",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 2833,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T04:13:51Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "18f51c38_45a1cd3c",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "ed9fde1e_6f979cfb",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4393,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-11T14:06:27Z",
      "side": 1,
      "message": "Similar to my other comment below: why it is safe to read p.status non-atomically?",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "55992c04_abd9d8a0",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4393,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-11T19:33:14Z",
      "side": 1,
      "message": "Why this change? We\u0027ve just read status in local variable s one line above.",
      "parentUuid": "ed9fde1e_6f979cfb",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "69d8b58b_dff70834",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4393,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T04:13:51Z",
      "side": 1,
      "message": "Remnant from an abandoned refactor. Fixed.",
      "parentUuid": "55992c04_abd9d8a0",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": false
    },
    {
      "key": {
        "uuid": "4a4515c6_b62c292a",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4399,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-11T19:50:32Z",
      "side": 1,
      "message": "Now that we start using goto\u0027s perhaps it\u0027s time to refactor it :)\n\nif s \u003d\u003d _Psyscall \u0026\u0026 retakep(_p_) {\n  continue\n}\nif s \u003d\u003d _Psyscall || s \u003d\u003d _Prunning {\n  ... preempt ...\n}",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "a976051f_10e9bd73",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4399,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T04:13:51Z",
      "side": 1,
      "message": "Ack. I tried to split this function up unsuccessfully a few times which is why I ended up with the goto. Extracting a function is complicated by the fact that we might need to drop and reacquire allpLock, and we also might need to update the state in p.sysmontick. That makes for extremely non-obvious and single-purpose functions.\n\nI tried a bit harder and was able to refactor the goto into a switch. Let me know what you think.",
      "parentUuid": "4a4515c6_b62c292a",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "1a6c3750_69483f5f",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4441,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-11T19:50:32Z",
      "side": 1,
      "message": "Why? This looks harmful for performance and unnecessary.",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "9f8efbd7_bad6e9f4",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4441,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T04:13:51Z",
      "side": 1,
      "message": "So with this line the new test takes 130ms ± 5ms. That matches my expectations: 100ms of real \"work\" in the main thread plus some scheduling overhead. But without this line the new test takes 1000ms ± 500ms. That\u0027s better, but falls short of my expectation.\n\nIt seemed to me that if sysmon is preempting goroutines, that should count as sysmon doing work and thus reset its idle counter. Perhaps that\u0027s misguided, though, because goroutines are only preempted after 10ms (the max sysmon tick) while syscalls can be preempted after 20us (the min sysmon tick). Are there scheduler benchmarks somewhere that I can run to determine the impact of this change on other workloads?\n\nThe core problem to be fixed here is that if sysmon ends up with a 10ms tick resolution, it will fail to retake Ps in 20us like it \"normally\" would. Another way to solve this would be to reset sysmon\u0027s tick resolution whenever entersyscall is called. I don\u0027t have a sense of whether that would be a reasonable thing to do. Does your average Go program wind up in entersyscall a lot? I could see the answer being no if some of the more common syscalls (like read/write) end up in entersyscallblock instead, but given that fetching the current time requires a syscall on many platforms, I could also see the answer being yes.",
      "parentUuid": "1a6c3750_69483f5f",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "bf3dfcb4_0f648fe1",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4441,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-12T19:33:53Z",
      "side": 1,
      "message": "At the very least this needs to go in a separate change. Even if it touches the same code, it does a very different thing from the rest of the change.\n\nBut I am still not convinced that it\u0027s the right thing to do.\nI believe this speeds up this test program, but we can\u0027t consider it in isolation.\n\nFirst, all other goroutines in the program are also \"real work\". The fact that a goroutine become runnable does not mean that we need to run it right away. If there are more than GOMAXPROCS such goroutines, we simply can\u0027t run all goroutines at the same time. Somebody will have to wait.\nYes, preemption granularity is 10ms. And that\u0027s why we let sysmon to cool down to 10ms if it only preempts goroutines. What you are trying to do is to piggy back on preemption that we have in this program to also speed up syscall retaking. But these are unrelated things. You can have the same syscall pattern but without preemption that will ensure that syscalls are handled faster (I suspect it\u0027s actually this same test but with GOMAXPROCS\u003d1). So this change is very much tailored to bitter specifics of this particular test. If we want to handle syscalls better, we need to handle syscalls better. We can\u0027t piggy back on preemption for this.\n\nYes, normal programs do lots of syscalls. Namely networking programs doing read/write, which are non-blocking and do reentersyscall and normally take ~~5us. It\u0027s intended that sysmon cools down to 10ms for such programs.\n\nRe benchmarks, I don\u0027t have any good to offer. It\u0027s hard to measure.\nBut here is some data. This is percent of CPU taken by \"for (;;) usleep(X)\" C program for different values of X, collected on latest linux kernel and a recent intel cpu:\n\n    64us -\u003e 4.0%\n   128us -\u003e 2.7%\n   256us -\u003e 3.5%\n   512us -\u003e 2.8%\n  1024us -\u003e 1.6%\n  2048us -\u003e 0.6%\n  4096us -\u003e 0.3%\n  8192us -\u003e 0.0%\n\nTens of microseconds sleep takes considerable cpu time. Not imagine that there are several dozens of Go programs running on the same machine at the same time, each of them with own sysmon.",
      "parentUuid": "9f8efbd7_bad6e9f4",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "7f1f665d_13eb07fa",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4441,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-12T19:41:06Z",
      "side": 1,
      "message": "Also consider a single-goroutine computational program with no syscalls.",
      "parentUuid": "bf3dfcb4_0f648fe1",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "335530a6_df64eefb",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4441,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T22:12:26Z",
      "side": 1,
      "message": "Ack, I\u0027ll back this out of this CL. Some thoughts below, though.\n\n\u003e Also consider a single-goroutine computational program with no syscalls.\n\nIdeally we wouldn\u0027t even bother preempting if there\u0027s no additional work to be scheduled.\n\n\u003e What you are trying to do is to piggy back on preemption that we have in this program to also speed up syscall retaking. But these are unrelated things. \n\nThe current design of sysmon already tightly couples preemption with P-retaking. (I\u0027ll admit that this change would have made the coupling worse.) Checking whether goroutines need to be preempted every 20us is a waste of effort when the default preemption threshold is 10ms.\n\n\u003e Yes, normal programs do lots of syscalls. Namely networking programs doing read/write, which are non-blocking and do reentersyscall and normally take ~~5us. It\u0027s intended that sysmon cools down to 10ms for such programs.\n\nWhich is great until that program performs a syscall that takes 100ms instead of 5us. fsync is the example that comes to mind. When sysmon is running hot, the P that performs the fsync will be retaken in 20us. But if sysmon has cooled down, the P that performs the fsync might not be retaken for 10ms, which is rather a lot of CPU time to leave on the table.\n\nI\u0027d be happy to take a more principled approach if you\u0027re willing to offer me guidance. Perhaps we should be sampling some percentage of all syscalls and using that to set sysmon\u0027s tick interval?",
      "parentUuid": "7f1f665d_13eb07fa",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "68a656fd_64264672",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4441,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-16T23:57:40Z",
      "side": 1,
      "message": "\u003e Ack, I\u0027ll back this out of this CL. Some thoughts below, though.\n\u003e \n\u003e \u003e Also consider a single-goroutine computational program with no syscalls.\n\u003e \n\u003e Ideally we wouldn\u0027t even bother preempting if there\u0027s no additional work to be scheduled.\n\nOK, then GOMAXPROCS+1 computational goroutines.\n\n\u003e \u003e What you are trying to do is to piggy back on preemption that we have in this program to also speed up syscall retaking. But these are unrelated things. \n\u003e \n\u003e The current design of sysmon already tightly couples preemption with P-retaking. (I\u0027ll admit that this change would have made the coupling worse.) Checking whether goroutines need to be preempted every 20us is a waste of effort when the default preemption threshold is 10ms.\n\nThey are coupled but not in this sense.\nThey are done in the same thread and at the same time, because we iterate over Ps and load their statuses anyway. So it just looks more work and performance hit to decouple them.\n\nBut we don\u0027t require actual preemption to handle syscalls, nor the other way around.\n\n\n\u003e \u003e Yes, normal programs do lots of syscalls. Namely networking programs doing read/write, which are non-blocking and do reentersyscall and normally take ~~5us. It\u0027s intended that sysmon cools down to 10ms for such programs.\n\u003e \n\u003e Which is great until that program performs a syscall that takes 100ms instead of 5us. fsync is the example that comes to mind. When sysmon is running hot, the P that performs the fsync will be retaken in 20us. But if sysmon has cooled down, the P that performs the fsync might not be retaken for 10ms, which is rather a lot of CPU time to leave on the table.\n\nThis does not look related to preemption.\n\n\u003e I\u0027d be happy to take a more principled approach if you\u0027re willing to offer me guidance. Perhaps we should be sampling some percentage of all syscalls and using that to set sysmon\u0027s tick interval?\n\nI don\u0027t know what\u0027s better way to do it.",
      "parentUuid": "335530a6_df64eefb",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "5f201edc_7f8d26e3",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4456,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-11T19:50:32Z",
      "side": 1,
      "message": "We don\u0027t want to preempt _Psyscall here too? I think we need to comment why. Is it because of retries in stopTheWorldWithSema/freezetheworld?",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "8879c832_c76e1b13",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4456,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T04:13:51Z",
      "side": 1,
      "message": "That\u0027s a good question. stopTheWorldWithSema and forEachP both retake Ps so preemption is redundant. freezetheworld, however, does not attempt to retake Ps. (Perhaps it should.)\n\nI agree that it\u0027s odd that a function named preemptall ignores Ps in Psyscall while a function named preemptone explicitly handles Ps in Psyscall.\n\nHere\u0027s a proposal: teach preemptall to preempt Ps in Psyscall, purely for consistency\u0027s sake, and, in another CL, teach freezetheworld to retake Ps. What do you think?",
      "parentUuid": "5f201edc_7f8d26e3",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "2fcde6a1_c7cddf9e",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4456,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-12T19:33:53Z",
      "side": 1,
      "message": "Does freezetheworld need to retake Ps? What\u0027s the scenario where it fails to stop a goroutine without retaking Ps?",
      "parentUuid": "8879c832_c76e1b13",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "715735ef_33b3a9ac",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4456,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T22:12:26Z",
      "side": 1,
      "message": "\u003e Does freezetheworld need to retake Ps? What\u0027s the scenario where it fails to stop a goroutine without retaking Ps?\n\nHere\u0027s an example:\n\n```\npackage main\n\n// #include \u003cunistd.h\u003e\nimport \"C\"\n\nimport \"time\"\n\nvar nilPtr *int\n\nfunc main() {\n  for i :\u003d 0; i \u003c 10; i++ {\n    go func() {\n      // time.Sleep(time.Millisecond)\n      C.usleep(1000)\n      _ \u003d *nilPtr\n    }()\n  }\n  time.Sleep(time.Hour)\n}\n```\n\nWhen the goroutines sleep via time.Sleep, you\u0027ll only ever see one segfault, because the first goroutine to segfault manages to freezeTheWorld before the other goroutines segfault. When the goroutines sleep via C.sleep, you\u0027ll see a bunch of segfaults, because goroutines don\u0027t check for a frozen world when resuming from a segfault.\n\nI\u0027ll admit I don\u0027t particularly care about this example. The output when multiple simultaneous segfaults occur is perfectly readable. But perhaps there is a similar but more compelling example.",
      "parentUuid": "2fcde6a1_c7cddf9e",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "4baeb39c_3267e318",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4456,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-16T23:57:40Z",
      "side": 1,
      "message": "Does runtime print \"goroutine running on other thread; stack unavailable\" in any of these cases?\nThat\u0027s the thing that we are trying to avoid with preemption in freezetheworld.",
      "parentUuid": "715735ef_33b3a9ac",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "86c949c0_be8063c7",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4482,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-11T14:06:27Z",
      "side": 1,
      "message": "I\u0027m kind of unclear on the synchronization here. Is it really safe to load p.m.ptr() non-atomically? And ditto for the load of p.oldm.ptr() that I\u0027ve added. Couldn\u0027t this P enter a syscall at any moment and set p.m \u003d 0? (This was true even before my last CL, since entersyscallblock has always called handoffp, which sets p.m \u003d 0.)\n\nI\u0027m not worried about reading a potentially stale value, as this function has a big \"might preempt the wrong goroutine\" warning label on it. I am worried, however, about observing only half of the pointer. Are we perhaps assuming that writes/reads of a pointer-sized value are always atomic?",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "a10f3ae8_e03de4a8",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4482,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-11T19:31:08Z",
      "side": 1,
      "message": "Historically we were sloppy about some atomic accesses because atomic ops were/are expensive (maybe also because of the \"C is portable assembly\" mindset).\nEffectively we consider and hope this to be at least a relaxed atomic load (i.e. gives atomicity but no memory ordering).",
      "parentUuid": "86c949c0_be8063c7",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "1aaa9160_57e04b71",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4482,
      "author": {
        "id": 5400
      },
      "writtenOn": "2018-11-11T19:33:14Z",
      "side": 1,
      "message": "There was a recent discussion of this here:\nhttps://go-review.googlesource.com/c/go/+/142277",
      "parentUuid": "a10f3ae8_e03de4a8",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    },
    {
      "key": {
        "uuid": "5e39c1cb_00cbdb14",
        "filename": "src/runtime/proc.go",
        "patchSetId": 1
      },
      "lineNbr": 4482,
      "author": {
        "id": 25418
      },
      "writtenOn": "2018-11-12T04:13:51Z",
      "side": 1,
      "message": "Thanks, that\u0027s helpful context. Sounds like this is OK then. I guess the Go compiler doesn\u0027t reorder memory load/stores the way that a C++ compiler does.",
      "parentUuid": "1aaa9160_57e04b71",
      "revId": "fd99053a54fda5771b3873ce3f24eff75d26a65f",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705",
      "unresolved": true
    }
  ]
}