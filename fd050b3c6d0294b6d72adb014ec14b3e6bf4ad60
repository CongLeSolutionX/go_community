{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "bb788891_6ee8529a",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 7
      },
      "lineNbr": 0,
      "author": {
        "id": 9210
      },
      "writtenOn": "2024-11-18T23:34:10Z",
      "side": 1,
      "message": "Thank you for the review! I\u0027ve sent a few CLs for the changes you suggested (mentioned inline), and I plan to also write patches for the two state word changes (unlock2 sets spinning bit, and mutexLocked protects waiter list). I\u0027ll remove the osyield call at the start of the next cycle.",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "fc5b35cd_da3dd982",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 164,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-18T15:30:08Z",
      "side": 1,
      "message": "v8 :\u003d ?",
      "fixSuggestions": [
        {
          "fixId": "7b776e19_13f5d24c",
          "description": "prompt_to_edit API",
          "replacements": [
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 162,
                "startChar": 0,
                "endLine": 163,
                "endChar": 0
              },
              "replacement": ""
            },
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 164,
                "startChar": 0,
                "endLine": 165,
                "endChar": 0
              },
              "replacement": "\tv8 :\u003d atomic.Xchg8(k8, mutexLocked)\n"
            }
          ]
        }
      ],
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "90daec04_c0e85560",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 164,
      "author": {
        "id": 9210
      },
      "writtenOn": "2024-11-18T23:34:10Z",
      "side": 1,
      "message": "Done, in CL 629415",
      "parentUuid": "fc5b35cd_da3dd982",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b24934eb_f43d1f46",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 173,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-19T08:43:32Z",
      "side": 1,
      "message": "I would consider doing this only before semasleep. I don\u0027t think it makes sense to track latency when the thread wasn\u0027t sleeping, we know it\u0027s very brief periods only.\n\nWe could also initialize spinning count in a global variable, so that we don\u0027t need a branch on every mutex lock.\n\nAlso restructure the code to completely remove \"withRank\" overhead. Currently there is still an additional thunk function call on every lock/unlock.",
      "fixSuggestions": [
        {
          "fixId": "9c930b52_84503eea",
          "description": "prompt_to_edit API",
          "replacements": [
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 173,
                "startChar": 0,
                "endLine": 175,
                "endChar": 0
              },
              "replacement": ""
            },
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 193,
                "startChar": 0,
                "endLine": 194,
                "endChar": 0
              },
              "replacement": ""
            },
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 199,
                "startChar": 0,
                "endLine": 200,
                "endChar": 0
              },
              "replacement": ""
            },
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 237,
                "startChar": 0,
                "endLine": 237,
                "endChar": 0
              },
              "replacement": "\t\t\t\ttimer :\u003d \u0026lockTimer{lock: l}\n\t\t\t\ttimer.begin()\n"
            },
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 238,
                "startChar": 0,
                "endLine": 238,
                "endChar": 0
              },
              "replacement": "\t\t\t\ttimer.end()\n"
            }
          ]
        }
      ],
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "23824226_37e17027",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 188,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-18T15:30:08Z",
      "side": 1,
      "message": "Can this be simplified? Is this \"v \u0026^ mutexSpinning\"?",
      "fixSuggestions": [
        {
          "fixId": "103ec70c_7fa4b433",
          "description": "prompt_to_edit API",
          "replacements": [
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 188,
                "startChar": 0,
                "endLine": 189,
                "endChar": 0
              },
              "replacement": "\t\t\t\tnext :\u003d (v \u0026^ mutexMMask) | (v \u0026^ mutexSpinning) | mutexLocked\n"
            }
          ]
        }
      ],
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "e1cf3acd_c32bb6ac",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 188,
      "author": {
        "id": 9210
      },
      "writtenOn": "2024-11-18T23:34:10Z",
      "side": 1,
      "message": "Done, in CL 629415",
      "parentUuid": "23824226_37e17027",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2f5f512b_fb89a4d1",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 197,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-18T15:46:24Z",
      "side": 1,
      "message": "Why do we unconditionally set mutexSleeping here?\nIs there real benefit of using Xchg8 instead of Casuintptr here? It looks like it only makes the code harder to reason.",
      "fixSuggestions": [
        {
          "fixId": "be93f920_0d131d53",
          "description": "prompt_to_edit API",
          "replacements": [
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 197,
                "startChar": 0,
                "endLine": 199,
                "endChar": 0
              },
              "replacement": "\t\t\t\tv :\u003d atomic.Loaduintptr(\u0026l.key)\n\t\t\t\tif v\u0026mutexLocked \u003d\u003d 0 {\n\t\t\t\t\ttimer.end()\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif atomic.Casuintptr(\u0026l.key, v, v|mutexLocked|mutexSleeping) {\n"
            },
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 203,
                "startChar": 0,
                "endLine": 204,
                "endChar": 0
              },
              "replacement": ""
            }
          ]
        }
      ],
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f4608ba6_d1f58ebe",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 197,
      "author": {
        "id": 9210
      },
      "writtenOn": "2024-11-18T23:34:10Z",
      "side": 1,
      "message": "I\u0027ve sent CL 629416 to try that. It\u0027s slower on the microbenchmark. I don\u0027t have a full understanding of what makes the CAS so much more expensive. Do you think it\u0027s worth a deeper look?",
      "parentUuid": "2f5f512b_fb89a4d1",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "8256afc3_ac390439",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 213,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-18T15:30:08Z",
      "side": 1,
      "message": "On Nahalem processors PAUSE latency has increased from few cycles to 140 cycles:\nhttps://www.reddit.com/r/hardware/comments/8s011f/skylakex_cpus_have_140cycle_pause_latency_with/?rdt\u003d48638\nhttps://stackoverflow.com/questions/74295100/whats-a-good-alternative-to-pause-for-use-in-the-implementation-of-a-spinlock\n\nSo doing 30 of these takes 4200 cycles. I would say that\u0027s way too much, it should be 1 at most.\n\nFWIW Absl Mutex (which is Google best take on how a production Mutex should work) does not use PAUSE entirely.",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c79458af_f69bc01a",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 213,
      "author": {
        "id": 9210
      },
      "writtenOn": "2024-11-18T23:34:10Z",
      "side": 1,
      "message": "I\u0027ve sent CL 629417 to try reducing this to `procyield(1)`. It seems to result in the mutex ownership moving between threads more often, which helps on fairness but hurts throughput. I\u0027ll run the perf builders.",
      "parentUuid": "8256afc3_ac390439",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "2b0e1f8e_68ee4c91",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 217,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-18T15:30:08Z",
      "side": 1,
      "message": "+1\nPassive spinning tend to create more problem than solve. Modern OSes are pretty good at low latency wakeups nowadays.",
      "fixSuggestions": [
        {
          "fixId": "e703af94_72fc6f72",
          "description": "prompt_to_edit API",
          "replacements": [
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 217,
                "startChar": 0,
                "endLine": 218,
                "endChar": 0
              },
              "replacement": "\t\t\t\tosyield() // TODO: Consider removing this step. See https://go.dev/issue/69268.\n"
            }
          ]
        }
      ],
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b3b0bb2f_27d5fc03",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 217,
      "author": {
        "id": 9210
      },
      "writtenOn": "2024-11-18T23:34:10Z",
      "side": 1,
      "message": "Thanks! My measurements so far have shown that removing osyield is fine, but -- in case it\u0027s necessary for some unusual workload -- I\u0027d prefer to leave this part of lock2\u0027s behavior in place until the Go 1.N+1 release.",
      "parentUuid": "2b0e1f8e_68ee4c91",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "06166200_163f8baf",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 224,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-18T15:46:24Z",
      "side": 1,
      "message": "Here we loop until we enqueue the current M into the wait list, right?\nWhat if this M is not weSpin, but there is another M that was weSpin and just acquired the mutex, don\u0027t we want this M to become spinning instead? I would assume we want that.\n\nThe code may be simpler, it\u0027s structured w/o nested loops as:\n\n```\ntryAcquire:\nfor i :\u003d 0; ; i++ {\n\tv \u003d atomic.Loaduintptr(\u0026l.key)\n\n\tif ... {\n\t\tif CAS(...) {\n\t\t\treturn\n\t\t}\n\t} else if ... {\n\t\tif CAS(...) {\n\t\t\treturn\n\t\t}\n\t}\n}\n```\n\nIt would make it simpler to reason about state transitions.",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "96c95f79_f5d9ac1a",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 224,
      "author": {
        "id": 9210
      },
      "writtenOn": "2024-11-18T23:34:10Z",
      "side": 1,
      "message": "Done, in CL 629415",
      "parentUuid": "06166200_163f8baf",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "892d982a_bb910af8",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 235,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-18T15:30:08Z",
      "side": 1,
      "message": "A common optimization in mutexes is so called thundering herd problem avoidance.\nConsider that a wait list of, say, 10 Ms has built up.\nNow another M locks/unlocks the mutex in a tight loop (e.g. adds a bunch of timers or something). On each unlock it will wake another M from the the wait list, even though none of them is actually scheduled and had a chance to acquire the mutex.\nThe prevention for that is always waking with weSpin \u003d true, and make the unlocking M to set mutexSpinning on our behalf. When an M is woken, we do know that it will try to acquire the mutex soon, so there is no need to wake more Ms until the first one tried to. This helps to auto-tune contention level on heavily contended mutexes.\n\nThough, for mutexPreferLowLatency it may be counter-productive. A bit more complex solution is to allocate several bits for mutexSpinning and use them as a saturating counter that will account both spinning and woken threads. It would allow to trade some CPU cycles for lower latency.\n\nBut since it\u0027s more complex, just waking with weSpin may be a good first step.",
      "fixSuggestions": [
        {
          "fixId": "f2208cd2_34677669",
          "description": "prompt_to_edit API",
          "replacements": [
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 65,
                "startChar": 0,
                "endLine": 68,
                "endChar": 0
              },
              "replacement": "\tmutexActiveSpinCount \u003d 4\n\tmutexActiveSpinSize  \u003d 30\n"
            },
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 216,
                "startChar": 0,
                "endLine": 217,
                "endChar": 0
              },
              "replacement": "\t\t\t} else if i \u003c spin+1 {\n"
            }
          ]
        }
      ],
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "4a7e38b0_6f9efc6a",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 235,
      "author": {
        "id": 9210
      },
      "writtenOn": "2024-11-18T23:34:10Z",
      "side": 1,
      "message": "I tried something similar in CL 622997 (unlock2 sets the spinning bit). That CL changed a few other aspects as well (unlock2 woke the longest-sleeping / tail thread, rather than the most recently sleeping / head thread), and overall had unimpressive performance.\n\nMy interpretation was that CL\u0027s performance was that if the unlocker sets the spinning bit, then any new entrants to lock2 would be unable to spin. There\u0027d be no spinning at all until a specific thread awoke from sleep, which could take a while.\n\nI made an attempt to have two spinning bits, one that was set in unlock2 on behalf of the thread it was about to wake, and another that could be claimed by any currently-awake thread. But I wasn\u0027t sure if the complexity would be worthwhile, and the performance results may have been muddied by the other changes in that CL (the decision to wake the longest sleeper, the decision to keep threads in the list until they leave lock2 rather than only while they\u0027re asleep, all of the logic on direct handoffs).\n\nI\u0027ll prototype with unlock2 setting the spinning bit, and with that plus a secondary bit that can be claimed by newcomers.",
      "parentUuid": "892d982a_bb910af8",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "c5dd67ef_fd4b114f",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 235,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-19T08:20:36Z",
      "side": 1,
      "message": "The same note about mutex benchmarks. They lie.\n\nsemasleep/semawakeup are considerable amount of local work. So doing them more will falsely benefit synthetic benchmark b/c other threads will be able to do longer \"streaks\" faster, while these threads distracted doing unuseful local work.",
      "parentUuid": "4a7e38b0_6f9efc6a",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "698e119d_70e62b99",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 294,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-18T15:30:08Z",
      "side": 1,
      "message": "This mutexStackLocked logic looks rather complex and requires 3 atomic RMW for unlock with waiters instead of 1.\n\nI would use the mutexLocked bit as \"we own the list\" bit. This requires using full word CAS loop for actual unlock. But it should be fine for unlock b/c the memory is likely to be cached on unlock. In exchange we could do all of unlock+waiter pop with 1 CAS (if it observes mutexSpinning, then it can just drop mutexLocked bit and return).",
      "fixSuggestions": [
        {
          "fixId": "21bb4f71_0a066040",
          "description": "prompt_to_edit API",
          "replacements": [
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 40,
                "startChar": 0,
                "endLine": 42,
                "endChar": 0
              },
              "replacement": "// Bit 9, mutexStackLocked, is a try-lock that grants an unlocking M permission to\n// inspect the list of waiting Ms and to pop an M off of that stack.\n"
            },
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 294,
                "startChar": 0,
                "endLine": 295,
                "endChar": 0
              },
              "replacement": "\t\tif v\u0026^mutexMMask \u003d\u003d 0 || v\u0026mutexLocked !\u003d 0 {\n"
            },
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 313,
                "startChar": 0,
                "endLine": 314,
                "endChar": 0
              },
              "replacement": "\t\tnext :\u003d v | mutexLocked\n"
            },
            {
              "path": "src/runtime/lock_spinbit.go",
              "range": {
                "startLine": 327,
                "startChar": 0,
                "endLine": 328,
                "endChar": 0
              },
              "replacement": "\t\tflags :\u003d v \u0026 (mutexMMask \u0026^ mutexLocked) // preserve low bits, but release stack lock\n"
            }
          ]
        }
      ],
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "5919afa7_3755d035",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 294,
      "author": {
        "id": 9210
      },
      "writtenOn": "2024-11-18T23:34:10Z",
      "side": 1,
      "message": "I will prepare a CL to try that. There are two reasons it\u0027s not in this revision.\n\nFirst, in the zero-contention case there\u0027s a clear speedup of Xchg8 rather than CAS. On my darwin/arm64 machine (an M1 MacBook Air, which used lock_sema.go\u0027s CAS-based implementation as a baseline), it\u0027s a bit over 12%.\n\nSecond, anything protected by mutexLocked is part of the critical section, so I\u0027m reluctant to add non-essential code to that path. As this CL stands, we\u0027re out of the critical section at this point and can skip all of the CAS operations if some other thread has already claimed mutexStackLocked.\n\nI wasn\u0027t able to get the performance to work in my earlier attempts, but I\u0027ll give it another try. There are certainly benefits in simplicity and in profile accuracy (outlined in the comment that follows).",
      "parentUuid": "698e119d_70e62b99",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "56ff63ac_3f164f3f",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 294,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-19T08:20:36Z",
      "side": 1,
      "message": "\u003e CAS-based implementation as a baseline), it\u0027s a bit over 12%.\n\nThis looks suspicious. Is it in contended or uncontended case?\n\nsemawakeup should definitely happen after unlock. The only additional expensive operation is loading of wakem.mWaitList.next. To avoid it, I would at least consider resetting  mutexLocked and setting mutexStackLocked with 1 CAS.",
      "parentUuid": "5919afa7_3755d035",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "033f26b8_e14e7936",
        "filename": "src/runtime/lock_spinbit.go",
        "patchSetId": 7
      },
      "lineNbr": 294,
      "author": {
        "id": 5400
      },
      "writtenOn": "2024-11-20T06:26:31Z",
      "side": 1,
      "message": "If we load the full work in unlock, can we get rid of the sleeping bit and the additional atomic.Or8 in lock?",
      "parentUuid": "56ff63ac_3f164f3f",
      "revId": "fd050b3c6d0294b6d72adb014ec14b3e6bf4ad60",
      "serverId": "62eb7196-b449-3ce5-99f1-c037f21e1705"
    }
  ]
}